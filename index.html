<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è­·ç†å­¸ç”Ÿå‚·å£è­·ç†è¨“ç·´ - Nursing Wound Care Training</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }

        .main-container {
            max-width: 1400px;
            margin: 0 auto;
            display: grid;
            grid-template-columns: 1fr 350px;
            gap: 20px;
        }

        .left-panel {
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
        }

        .right-panel {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }

        .info-card,
        .settings-card {
            background: white;
            border-radius: 15px;
            padding: 20px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }

        .header {
            text-align: center;
            margin-bottom: 30px;
        }

        .header h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 28px;
        }

        .header h2 {
            color: #34495e;
            margin-bottom: 15px;
            font-size: 18px;
        }

        .azure-badge {
            background: #0078d4;
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
            margin-left: 10px;
        }

        .ai-badge {
            background: #28a745;
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
            margin-left: 10px;
        }

        .scenario-info {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }

        .patient-info {
            background: #e8f5e8;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #27ae60;
        }

        .patient-info h4 {
            margin-top: 0;
            color: #155724;
        }

        .controls {
            display: flex;
            gap: 15px;
            justify-content: center;
            margin: 25px 0;
            flex-wrap: wrap;
        }

        .btn {
            padding: 12px 24px;
            border: none;
            border-radius: 8px;
            font-size: 16px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: 600;
        }

        .btn-primary {
            background: #3498db;
            color: white;
        }

        .btn-primary:hover {
            background: #2980b9;
            transform: translateY(-2px);
        }

        .btn-success {
            background: #28a745;
            color: white;
        }

        .btn-success:hover {
            background: #218838;
        }

        .btn-danger {
            background: #e74c3c;
            color: white;
        }

        .btn-danger:hover {
            background: #c0392b;
        }

        .btn:disabled {
            background: #bdc3c7;
            cursor: not-allowed;
            transform: none;
        }

        .btn-small {
            padding: 8px 16px;
            font-size: 14px;
        }

        .status {
            text-align: center;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            font-weight: 600;
        }

        .status.loading {
            background: #fff3cd;
            color: #856404;
        }

        .status.ready {
            background: #d4edda;
            color: #155724;
        }

        .status.recording {
            background: #f8d7da;
            color: #721c24;
            animation: pulse 1.5s infinite;
        }

        .status.error {
            background: #f8d7da;
            color: #721c24;
        }

        @keyframes pulse {

            0%,
            100% {
                opacity: 1;
            }

            50% {
                opacity: 0.7;
            }
        }

        .timer {
            font-size: 24px;
            font-weight: bold;
            text-align: center;
            color: #2c3e50;
            margin: 20px 0;
        }

        .conversation {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            max-height: 400px;
            overflow-y: auto;
        }

        .message {
            margin: 15px 0;
            padding: 15px;
            border-radius: 8px;
            line-height: 1.5;
        }

        .student-message {
            background: #e3f2fd;
            margin-left: 20px;
            border-left: 4px solid #2196f3;
        }

        .patient-message {
            background: #fff3e0;
            margin-right: 20px;
            border-left: 4px solid #ff9800;
        }

        .message-header {
            font-weight: bold;
            margin-bottom: 8px;
            color: #2c3e50;
        }

        .message-content {
            font-size: 16px;
        }

        .cantonese-text {
            color: #c0392b;
            font-weight: 500;
        }

        .procedure-notes {
            background: #f0f8ff;
            padding: 10px;
            margin: 10px 0;
            border-radius: 6px;
            font-style: italic;
            color: #34495e;
            border-left: 3px solid #3498db;
        }

        .download-section {
            text-align: center;
            margin-top: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
        }

        .loading-spinner {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid #f3f3f3;
            border-top: 3px solid #3498db;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            0% {
                transform: rotate(0deg);
            }

            100% {
                transform: rotate(360deg);
            }
        }

        .settings-card h4 {
            margin-top: 0;
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }

        .setting-group {
            margin: 15px 0;
        }

        .setting-group label {
            display: block;
            margin-bottom: 5px;
            font-weight: 600;
            color: #34495e;
        }

        .setting-group select {
            width: 100%;
            padding: 8px;
            border-radius: 6px;
            border: 1px solid #ddd;
            font-size: 14px;
        }

        .audio-status {
            padding: 10px;
            border-radius: 6px;
            margin: 10px 0;
            font-weight: bold;
            text-align: center;
        }

        .audio-status.success {
            background: #d4edda;
            color: #155724;
        }

        .audio-status.warning {
            background: #fff3cd;
            color: #856404;
        }

        .audio-status.error {
            background: #f8d7da;
            color: #721c24;
        }

        .volume-control {
            margin: 10px 0;
        }

        .volume-control input[type="range"] {
            width: 100%;
            margin: 5px 0;
        }

        .debug-section {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #dc3545;
            font-family: monospace;
            font-size: 12px;
            max-height: 200px;
            overflow-y: auto;
        }

        .debug-section h4 {
            color: #dc3545;
            margin-top: 0;
        }

        .debug-log {
            background: #f8f9fa;
            padding: 5px;
            margin: 5px 0;
            border-radius: 4px;
            color: #6c757d;
        }

        .debug-error {
            background: #f8d7da;
            color: #721c24;
        }

        .debug-info {
            background: #d1ecf1;
            color: #0c5460;
        }

        .ai-status {
            background: #e8f5e8;
            padding: 10px;
            border-radius: 6px;
            margin: 10px 0;
            font-weight: bold;
            text-align: center;
            border-left: 4px solid #28a745;
        }

        .ai-status.loading {
            background: #fff3cd;
            color: #856404;
            border-left-color: #ffc107;
        }

        .ai-status.ready {
            background: #d4edda;
            color: #155724;
            border-left-color: #28a745;
        }

        .ai-status.error {
            background: #f8d7da;
            color: #721c24;
            border-left-color: #dc3545;
        }

        @media (max-width: 1200px) {
            .main-container {
                grid-template-columns: 1fr;
                max-width: 900px;
            }

            .right-panel {
                flex-direction: row;
                flex-wrap: wrap;
            }

            .info-card,
            .settings-card {
                flex: 1;
                min-width: 300px;
            }
        }

        @media (max-width: 768px) {
            .right-panel {
                flex-direction: column;
            }

            .controls {
                flex-direction: column;
                align-items: center;
            }

            .btn {
                width: 100%;
                max-width: 300px;
            }
        }
    </style>
</head>

<body>
    <div class="main-container">
        <div class="left-panel">
            <div class="header">
                <h1>ğŸ¥ è­·ç†å­¸ç”Ÿå‚·å£è­·ç†è¨“ç·´ç³»çµ± <span class="azure-badge">Azure AI</span><span
                        class="ai-badge">Transformer.js</span></h1>
                <h2>Professional Nursing Student Wound Care Training System</h2>
                <p>Advanced AI-powered patient simulation with intelligent responses and realistic Cantonese voices</p>
            </div>

            <div class="scenario-info">
                <h3>ğŸ“‹ è¨“ç·´å ´æ™¯ Training Scenario</h3>
                <p><strong>ä½ çš„è§’è‰² Your Role:</strong> XXå¤§å­¸è­·ç†å­¸ç”Ÿä½•åŒå­¸ (Nursing Student Ho from XX University)</p>
                <p><strong>ä»»å‹™ Task:</strong> é€²è¡Œå‚·å£è­·ç†ç¨‹åºä¸¦èˆ‡ç—…äººä¿æŒè‰¯å¥½æºé€š</p>
                <p><strong>é‡é» Focus:</strong> èº«ä»½ç¢ºèªã€ç¨‹åºè§£é‡‹ã€ç—…äººå®‰æ’«ã€å°ˆæ¥­æºé€š</p>
            </div>

            <div id="aiStatus" class="ai-status loading">
                <span class="loading-spinner"></span> æ­£åœ¨åŠ è¼‰AIæ¨¡å‹... Loading AI model...
            </div>

            <div id="status" class="status loading">
                <span class="loading-spinner"></span> æ­£åœ¨åˆå§‹åŒ–AzureèªéŸ³æœå‹™... Initializing Azure Speech Services...
            </div>

            <div class="controls">
                <button id="startBtn" class="btn btn-primary" disabled>ğŸ™ï¸ é–‹å§‹è¨“ç·´ Start Training</button>
                <button id="stopBtn" class="btn btn-danger" disabled>â¹ï¸ çµæŸè¨“ç·´ Stop Training</button>
            </div>

            <div id="timer" class="timer">00:00</div>

            <div id="conversation" class="conversation" style="display: none;">
                <h4>ğŸ’¬ å°è©±è¨˜éŒ„ Session Conversation:</h4>
                <div id="messages"></div>
            </div>

            <div id="downloadSection" class="download-section" style="display: none;">
                <h4>ğŸ“¥ è¨“ç·´è¨˜éŒ„ Session Recording</h4>
                <p>ä½ çš„è¨“ç·´å°è©±å·²è¢«è¨˜éŒ„ä»¥ä¾›åˆ†æ Your training session has been recorded for analysis.</p>
                <button id="downloadBtn" class="btn btn-primary">ä¸‹è¼‰éŒ„éŸ³ Download Recording</button>
            </div>

            <div id="debugSection" class="debug-section" style="display: none;">
                <h4>ğŸ”§ Debug Information</h4>
                <div id="debugLogs"></div>
                <div style="margin-top: 10px;">
                    <button onclick="clearDebugLogs()" class="btn btn-primary btn-small">Clear Logs</button>
                    <button onclick="toggleDebug()" class="btn btn-primary btn-small">Hide Debug</button>
                </div>
            </div>

            <div class="controls">
                <button onclick="toggleDebug()" class="btn btn-primary btn-small">ğŸ”§ Toggle Debug</button>
            </div>
        </div>

        <div class="right-panel">
            <div class="info-card">
                <h4>ğŸ‘¨ ç—…äººè³‡æ–™ Patient Information</h4>
                <div class="patient-info">
                    <p><strong>å§“å Name:</strong> æ–‡åŠ›æ–°å…ˆç”Ÿ (Mr. Man Lik Sun)</p>
                    <p><strong>HN Number:</strong> 23332099</p>
                    <p><strong>å¹´é½¡ Age:</strong> 65æ­²</p>
                    <p><strong>èªè¨€ Language:</strong> å»£æ±è©±/è‹±æ–‡ (Cantonese/English Mix)</p>
                    <p><strong>ç—…æƒ… Condition:</strong> è…¿éƒ¨å‚·å£éœ€è¦æ¸…æ´—åŠæ›è—¥ (Leg wound requiring cleaning and dressing)</p>
                    <p><strong>æ€§æ ¼ç‰¹é»:</strong> å¥è«‡ä½†å®¹æ˜“ç·Šå¼µï¼Œå–œæ­¡è©³ç´°è§£é‡‹</p>
                </div>
            </div>

            <div class="settings-card">
                <h4>ğŸ™ï¸ èªéŸ³è¨­å®š Voice Settings</h4>

                <div class="setting-group">
                    <label>Patient Voice:</label>
                    <select id="voiceSelect">
                        <option value="zh-HK-HiuMaanNeural">HiuMaan (Female, Elderly)</option>
                        <option value="zh-HK-WanLungNeural">WanLung (Male, Elderly)</option>
                        <option value="zh-HK-HiuGaaiNeural">HiuGaai (Female, Young)</option>
                    </select>
                </div>

                <div class="setting-group">
                    <label>Speech Recognition:</label>
                    <select id="recognitionLang">
                        <option value="zh-HK">å»£æ±è©± Cantonese (Hong Kong)</option>
                        <option value="en-US">English (US)</option>
                        <option value="zh-CN">æ™®é€šè©± Mandarin</option>
                    </select>
                </div>

                <h4>ğŸ”Š éŸ³é »æ¸¬è©¦ Audio Test</h4>

                <div id="audioStatus" class="audio-status warning">
                    ç­‰å¾…éŸ³é »æ¸¬è©¦... Waiting for audio test...
                </div>

                <div class="volume-control">
                    <label for="volumeSlider">éŸ³é‡ Volume:</label>
                    <input type="range" id="volumeSlider" min="0" max="1" step="0.1" value="0.8">
                    <div style="text-align: center; margin-top: 5px;">
                        <span id="volumeDisplay">80%</span>
                    </div>
                </div>

                <div style="display: flex; flex-direction: column; gap: 10px; margin-top: 15px;">
                    <button onclick="testAudio()" class="btn btn-success btn-small">ğŸµ æ¸¬è©¦éŸ³é » Test Audio</button>
                    <button onclick="enableAudioContext()" class="btn btn-primary btn-small">ğŸ”“ å•Ÿç”¨éŸ³é » Enable
                        Audio</button>
                </div>
            </div>
        </div>
    </div>

    <script type="module">
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2';

        // Disable local model loading to use CDN
        env.allowRemoteModels = true;
        env.allowLocalModels = false;

        // Netlify Function Configuration
        const SPEECH_ENDPOINTS = [
            '/.netlify/functions/speech',
            '/api/speech'
        ];
        let CURRENT_ENDPOINT = SPEECH_ENDPOINTS[0];

        // Global variables
        let mediaRecorder;
        let audioChunks = [];
        let recognition;
        let sessionStartTime;
        let timerInterval;
        let isRecording = false;
        let conversationHistory = [];
        let selectedVoice = 'zh-HK-HiuMaanNeural';
        let selectedRecognitionLang = 'zh-HK';
        let debugMode = false;
        let audioContext = null;
        let currentVolume = 0.8;

        // AI Model variables
        let textGenerator = null;
        let isAIReady = false;

        // Patient personality and context
        const patientContext = {
            name: "æ–‡åŠ›æ–°",
            age: 65,
            condition: "è…¿éƒ¨å‚·å£éœ€è¦æ¸…æ´—åŠæ›è—¥",
            personality: "å¥è«‡ä½†å®¹æ˜“ç·Šå¼µï¼Œå–œæ­¡è©³ç´°è§£é‡‹ï¼Œæœƒæ··åˆå»£æ±è©±å’Œè‹±æ–‡èªªè©±",
            currentPain: 6,
            anxiety: 7,
            previousExperience: "ä¹‹å‰æœ‰åšéå‚·å£è­·ç†ï¼Œä½†æ¯æ¬¡éƒ½å¾ˆç·Šå¼µ"
        };

        // Initialize AI Model (using rule-based system for better Chinese support)
        async function initializeAI() {
            try {
                debugLog('Initializing AI conversation system...');
                updateAIStatus('ğŸ¤– æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½å°è©±ç³»çµ±... Initializing intelligent conversation system...', 'loading');

                // Use rule-based AI for better Chinese/Cantonese support
                // This avoids the character encoding issues with pre-trained English models
                textGenerator = {
                    initialized: true,
                    generateResponse: function (context, options) {
                        return Promise.resolve([{
                            generated_text: generateIntelligentResponse(context)
                        }]);
                    }
                };

                debugLog('AI conversation system loaded successfully');
                updateAIStatus('âœ… æ™ºèƒ½å°è©±ç³»çµ±æº–å‚™å°±ç·’ï¼Intelligent conversation system ready!', 'ready');
                isAIReady = true;

                return true;
            } catch (error) {
                debugLog(`AI initialization failed: ${error.message}`, 'error');
                updateAIStatus('âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•— System initialization failed', 'error');
                isAIReady = false;
                throw error;
            }
        }

        // Generate intelligent response using rule-based AI optimized for Chinese/Cantonese
        function generateIntelligentResponse(context) {
            const lines = context.split('\n');
            const lastStudentInput = lines[lines.length - 2]?.replace('è­·å£«: ', '') || '';
            const conversationHistory = lines.slice(2, -2); // Remove header and current input

            debugLog(`Generating response for: ${lastStudentInput}`);
            debugLog(`Conversation length: ${conversationHistory.length}`);

            // Analyze conversation context
            const analysis = analyzeConversationContext(lastStudentInput, conversationHistory);
            debugLog(`Context analysis: ${JSON.stringify(analysis)}`);

            // Generate contextual response
            return generateContextualResponse(analysis, lastStudentInput);
        }

        // Analyze conversation context for better responses
        function analyzeConversationContext(input, history) {
            const inputLower = input.toLowerCase();
            const historyText = history.join(' ').toLowerCase();

            return {
                phase: determineConversationPhase(history.length, historyText),
                emotion: detectEmotionalCues(inputLower),
                topic: identifyTopic(inputLower),
                hasIntroduced: historyText.includes('è­·å£«') || historyText.includes('ä½•'),
                hasIdentified: historyText.includes('æ–‡åŠ›æ–°') || historyText.includes('23332099'),
                isProcedureStarted: historyText.includes('å‚·å£') || historyText.includes('æ›è—¥')
            };
        }

        // Determine conversation phase
        function determineConversationPhase(historyLength, historyText) {
            if (historyLength <= 2) return 'greeting';
            if (historyLength <= 6 && !historyText.includes('æ–‡åŠ›æ–°')) return 'identification';
            if (historyLength <= 10 && !historyText.includes('å‚·å£')) return 'explanation';
            return 'procedure';
        }

        // Detect emotional cues
        function detectEmotionalCues(input) {
            if (input.includes('ç—›') || input.includes('pain') || input.includes('hurt')) return 'pain';
            if (input.includes('é©š') || input.includes('scared') || input.includes('afraid') || input.includes('nervous')) return 'anxiety';
            if (input.includes('å¥½') || input.includes('å””è©²') || input.includes('thank')) return 'positive';
            if (input.includes('å””æ˜') || input.includes('ä¸æ˜') || input.includes('understand')) return 'confused';
            return 'neutral';
        }

        // Identify conversation topic
        function identifyTopic(input) {
            if (input.includes('è­·å£«') || input.includes('ä½•') || input.includes('å')) return 'introduction';
            if (input.includes('æ–‡') || input.includes('å°å') || input.includes('23332099')) return 'identification';
            if (input.includes('å‚·å£') || input.includes('æ›è—¥') || input.includes('æ´—') || input.includes('wound')) return 'procedure';
            if (input.includes('ç—›') || input.includes('pain')) return 'pain_concern';
            return 'general';
        }

        // Generate contextual response based on analysis
        function generateContextualResponse(analysis, studentInput) {
            const responses = getResponsesByContext(analysis);

            // Add emotional modulation
            let selectedResponse = selectBestResponse(responses, analysis);

            // Add personality touches
            selectedResponse = addPersonalityElements(selectedResponse, analysis);

            debugLog(`Generated response: ${selectedResponse}`);
            return selectedResponse;
        }

        // Get appropriate responses based on context
        function getResponsesByContext(analysis) {
            const responseBank = {
                greeting: {
                    neutral: [
                        "å¥½å•Šï¼Œå…¥åšŸå•¦ã€‚ä½ å«å’©åå‘€ï¼Ÿ",
                        "å‘€ï¼Ÿé‚Šå€‹å‘€ï¼Ÿä¿‚è­·å£«åšŸã—ï¼ŸWho is there?",
                        "å…¥åšŸå…¥åšŸï¼Œä½ ä¿‚é‚Šä½è­·å£«ï¼Ÿ"
                    ]
                },
                identification: {
                    neutral: [
                        "ä¿‚å•Šï¼Œæˆ‘ä¿‚æ–‡åŠ›æ–°ã€‚ä½ è¦åšå’©å‘€ï¼Ÿ",
                        "æˆ‘å€‹HNä¿‚23332099ã€‚é»è§£è¦checkã—ï¼Ÿ",
                        "æ–‡åŠ›æ–°ï¼Œ65æ­²ã€‚My leg is hurt. æœ‰å•²ç—›ã—ã€‚"
                    ],
                    positive: [
                        "å¥½å˜…ï¼Œæˆ‘ä¿‚æ–‡åŠ›æ–°ï¼ŒHN 23332099ã€‚ä½ è¦å¹«æˆ‘åšå’©ï¼Ÿ",
                        "å†‡å•é¡Œï¼Œæˆ‘ä¿‚æ–‡åŠ›æ–°ã€‚Thank you for checking."
                    ]
                },
                explanation: {
                    neutral: [
                        "æ´—å‚·å£ï¼Ÿè¦é»æ´—ã—ï¼Ÿæœƒå””æœƒå¥½ç—›ï¼Ÿ",
                        "æ›è—¥ä¿‚å’ªè¦é™¤å’—èˆŠå˜…å…ˆï¼Ÿæˆ‘æ‡‰è©²é»é…åˆï¼Ÿ",
                        "ä½ å¯å””å¯ä»¥è§£é‡‹è©³ç´°å•²ï¼ŸI want to understand."
                    ],
                    anxiety: [
                        "æ´—å‚·å£ï¼Ÿæœƒå””æœƒå¥½ç—›ã—ï¼ŸI am scared. æˆ‘æœ‰å•²é©šå‘€ã€‚",
                        "æˆ‘ä¹‹å‰æ´—å‚·å£éƒ½å¥½ç—›ã—ï¼Œä»Šæ¬¡æœƒå””æœƒå¥½å•²ï¼Ÿ",
                        "å¯å””å¯ä»¥è¼•æ‰‹å•²ï¼Ÿæˆ‘çœŸä¿‚å¥½æ€•ç—›ã€‚"
                    ],
                    positive: [
                        "å¥½å˜…ï¼Œæˆ‘æœƒé…åˆä½ ã€‚é»æ¨£åšæœ€å¥½ï¼Ÿ",
                        "å””è©²æ›¬ï¼ä½ è§£é‡‹å¾—å¥½æ¸…æ¥šã€‚"
                    ]
                },
                procedure: {
                    neutral: [
                        "è€Œå®¶é–‹å§‹å–‡ï¼Ÿæˆ‘æº–å‚™å¥½å–‡ã€‚",
                        "ä¿‚å’ªå’æ¨£ï¼Ÿæˆ‘åšå¾—å•±å””å•±ï¼Ÿ",
                        "ä¸‹ä¸€æ­¥è¦åšå’©ï¼Ÿ"
                    ],
                    pain: [
                        "ç—›å‘€ï¼å¯å””å¯ä»¥åœä¸€åœï¼Ÿ",
                        "æœ‰å•²ç—›ï¼Œä½†ä¿‚æˆ‘é ‚å¾—ä½ã€‚ç¹¼çºŒå•¦ã€‚",
                        "It hurts! ä½†ä¿‚æˆ‘çŸ¥é“è¦åšã—ã€‚"
                    ],
                    anxiety: [
                        "æˆ‘æœ‰å•²ç·Šå¼µï¼Œå¯å””å¯ä»¥æ…¢å•²ï¼Ÿ",
                        "æ·±å‘¼å¸...æ·±å‘¼å¸...æˆ‘å˜—è©¦æ”¾é¬†ã€‚",
                        "ä½ åšç·Šå’©ï¼Ÿå¯å””å¯ä»¥è©±æˆ‘çŸ¥ï¼Ÿ"
                    ]
                },
                general: {
                    confused: [
                        "å’©è©±ï¼Ÿæˆ‘è½å””æ¸…æ¥šå‘€ã€‚What did you say?",
                        "ä½ å¯å””å¯ä»¥è¬›æ…¢å•²ï¼ŸCan you speak slower?",
                        "å””å¥½æ„æ€ï¼Œæˆ‘å””ä¿‚å¥½æ˜ã€‚å¯å””å¯ä»¥å†è¬›ä¸€æ¬¡ï¼Ÿ"
                    ],
                    neutral: [
                        "ä¿‚å˜…ï¼Œæˆ‘è½ç·Šä½ è¬›ã€‚",
                        "å¥½å˜…ï¼Œç¹¼çºŒã€‚",
                        "æ˜ç™½ã€‚ç„¶å¾Œå‘¢ï¼Ÿ"
                    ]
                }
            };

            const phaseResponses = responseBank[analysis.phase] || responseBank.general;
            return phaseResponses[analysis.emotion] || phaseResponses.neutral || responseBank.general.neutral;
        }

        // Select best response with some randomization
        function selectBestResponse(responses, analysis) {
            // Add some personality-based selection logic
            let weightedResponses = responses;

            // Patient is talkative, prefer longer responses sometimes
            if (Math.random() > 0.7) {
                weightedResponses = responses.filter(r => r.length > 20);
                if (weightedResponses.length === 0) weightedResponses = responses;
            }

            return weightedResponses[Math.floor(Math.random() * weightedResponses.length)];
        }

        // Add personality elements to response
        function addPersonalityElements(response, analysis) {
            const personalityTouches = [
                // Nervous additions
                () => Math.random() > 0.8 ? response + " æˆ‘æœ‰å•²ç·Šå¼µå‘€ã€‚" : response,
                // English mixing
                () => Math.random() > 0.9 ? response + " You know what I mean?" : response,
                // Uncertainty
                () => Math.random() > 0.85 ? "å’..." + response : response,
                // Confirmation seeking
                () => Math.random() > 0.9 ? response + " ä¿‚å’ªï¼Ÿ" : response
            ];

            // Apply one random personality touch
            if (Math.random() > 0.6) {
                const touch = personalityTouches[Math.floor(Math.random() * personalityTouches.length)];
                return touch();
            }

            return response;
        }

        // Escape XML special characters for SSML
        function escapeXml(text) {
            return text.replace(/&/g, '&amp;')
                .replace(/</g, '&lt;')
                .replace(/>/g, '&gt;')
                .replace(/"/g, '&quot;')
                .replace(/'/g, '&apos;');
        }

        // Format response as SSML
        function formatResponseAsSSML(response) {
            const escapedResponse = escapeXml(response);
            return `<speak version="1.0" xml:lang="zh-HK">
                <voice name="${selectedVoice}">
                    <mstts:express-as style="chat">
                        ${escapedResponse}
                    </mstts:express-as>
                </voice>
            </speak>`;
        }

        // Fallback response system
        function generateFallbackResponse(studentInput) {
            const input = studentInput.toLowerCase();

            const responses = {
                greeting: [
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}"><mstts:express-as style="cheerful">å¥½å•Šï¼Œå…¥åšŸå•¦ã€‚ä½ å«å’©åå‘€ï¼ŸWhat is your name?</mstts:express-as></voice></speak>`,
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}"><mstts:express-as style="curious">é‚Šå€‹å‘€ï¼Ÿä¿‚è­·å£«åšŸã—ï¼ŸWho is there?</mstts:express-as></voice></speak>`
                ],
                identification: [
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}">ä¿‚å•Šï¼Œæˆ‘ä¿‚æ–‡åŠ›æ–°ã€‚ä½ è¦åšå’©å‘€ï¼ŸMy leg is hurt. æœ‰å•²ç—›ã—ã€‚</voice></speak>`,
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}">æˆ‘å€‹HNä¿‚23332099ã€‚é»è§£è¦checkã—ï¼Ÿ</voice></speak>`
                ],
                procedure: [
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}"><mstts:express-as style="fearful">æ´—å‚·å£ï¼Ÿæœƒå””æœƒå¥½ç—›ã—ï¼ŸI am scared. æˆ‘æœ‰å•²é©šå‘€ã€‚</mstts:express-as></voice></speak>`,
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}">ä¿‚å’ªè¦é™¤å’—å•²é‡å…ˆï¼Ÿè¦å””è¦æˆ‘éƒï¼Ÿ</voice></speak>`
                ],
                default: [
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}">å’©è©±ï¼Ÿæˆ‘è½å””æ¸…æ¥šå‘€ã€‚What did you say?</voice></speak>`,
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}">ä½ å¯å””å¯ä»¥è¬›æ…¢å•²ï¼ŸCan you speak slower?</voice></speak>`,
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}">å””å¥½æ„æ€ï¼Œæˆ‘å””ä¿‚å¥½æ˜ã€‚Sorry, I don't understand.</voice></speak>`
                ]
            };

            // Simple keyword matching for fallback
            if (input.includes('è­·å£«') || input.includes('ä½•') || input.includes('å…¥åšŸ')) {
                return responses.greeting[Math.floor(Math.random() * responses.greeting.length)];
            } else if (input.includes('æ–‡ç”Ÿ') || input.includes('å°å') || input.includes('æ–‡åŠ›æ–°') || input.includes('23332099')) {
                return responses.identification[Math.floor(Math.random() * responses.identification.length)];
            } else if (input.includes('æ´—å‚·å£') || input.includes('æ›è—¥') || input.includes('å””å¥½éƒ') || input.includes('procedure')) {
                return responses.procedure[Math.floor(Math.random() * responses.procedure.length)];
            } else {
                return responses.default[Math.floor(Math.random() * responses.default.length)];
            }
        }

        // Update AI status display
        function updateAIStatus(message, type) {
            const aiStatus = document.getElementById('aiStatus');
            aiStatus.innerHTML = type === 'loading' ?
                `<span class="loading-spinner"></span> ${message}` :
                message;
            aiStatus.className = `ai-status ${type}`;
        }

        // Audio context initialization for autoplay policy
        function enableAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                debugLog('Audio context created');
            }

            if (audioContext.state === 'suspended') {
                audioContext.resume().then(() => {
                    debugLog('Audio context resumed');
                    updateAudioStatus('ğŸ”Š éŸ³é »å·²å•Ÿç”¨ Audio enabled', 'success');
                }).catch(error => {
                    debugLog(`Audio context resume failed: ${error.message}`, 'error');
                    updateAudioStatus('âŒ éŸ³é »å•Ÿç”¨å¤±æ•— Audio enable failed', 'error');
                });
            } else {
                updateAudioStatus('âœ… éŸ³é »å·²å°±ç·’ Audio ready', 'success');
            }
        }

        // Test Azure connection endpoints
        async function testAzureConnection() {
            for (const endpoint of SPEECH_ENDPOINTS) {
                try {
                    debugLog(`Testing endpoint: ${endpoint}`);
                    const testSSML = `<speak version="1.0" xml:lang="zh-HK">
                        <voice name="zh-HK-HiuMaanNeural">æ¸¬è©¦</voice>
                    </speak>`;

                    const response = await fetch(endpoint, {
                        method: 'POST',
                        headers: {
                            'Content-Type': 'application/json'
                        },
                        body: JSON.stringify({ ssml: testSSML })
                    });

                    if (response.ok) {
                        const data = await response.json();
                        if (data.audio) {
                            CURRENT_ENDPOINT = endpoint;
                            debugLog(`Using endpoint: ${endpoint}`);
                            return true;
                        }
                    }
                } catch (error) {
                    debugLog(`Endpoint ${endpoint} failed: ${error.message}`, 'error');
                    continue;
                }
            }
            throw new Error('All Azure endpoints failed');
        }

        // Test audio function
        async function testAudio() {
            try {
                debugLog('Testing audio with simple TTS...');
                updateAudioStatus('ğŸ”„ æ­£åœ¨æ¸¬è©¦éŸ³é »... Testing audio...', 'warning');

                const testSSML = `<speak version="1.0" xml:lang="zh-HK">
                    <voice name="${selectedVoice}">
                        <mstts:express-as style="cheerful">
                            ä½ å¥½ï¼éŸ³é »æ¸¬è©¦æˆåŠŸï¼Hello! Audio test successful!
                        </mstts:express-as>
                    </voice>
                </speak>`;

                await speakPatientResponseAzure(testSSML, true);
                updateAudioStatus('âœ… éŸ³é »æ¸¬è©¦æˆåŠŸï¼Audio test successful!', 'success');

            } catch (error) {
                debugLog(`Audio test failed: ${error.message}`, 'error');
                updateAudioStatus('âŒ éŸ³é »æ¸¬è©¦å¤±æ•— Audio test failed', 'error');
            }
        }

        // Update audio status display
        function updateAudioStatus(message, type) {
            const audioStatus = document.getElementById('audioStatus');
            audioStatus.textContent = message;
            audioStatus.className = `audio-status ${type}`;
        }

        // Volume control
        const volumeSlider = document.getElementById('volumeSlider');
        const volumeDisplay = document.getElementById('volumeDisplay');

        volumeSlider.addEventListener('input', (e) => {
            currentVolume = parseFloat(e.target.value);
            volumeDisplay.textContent = Math.round(currentVolume * 100) + '%';
            debugLog(`Volume set to: ${Math.round(currentVolume * 100)}%`);
        });

        // Debug logging function
        function debugLog(message, type = 'info') {
            console.log(`[${type.toUpperCase()}] ${message}`);

            if (debugMode) {
                const debugLogs = document.getElementById('debugLogs');
                const logDiv = document.createElement('div');
                logDiv.className = `debug-log debug-${type}`;
                logDiv.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;
                debugLogs.appendChild(logDiv);
                debugLogs.scrollTop = debugLogs.scrollHeight;
            }
        }

        function clearDebugLogs() {
            document.getElementById('debugLogs').innerHTML = '';
        }

        function toggleDebug() {
            debugMode = !debugMode;
            const debugSection = document.getElementById('debugSection');
            debugSection.style.display = debugMode ? 'block' : 'none';
        }

        // DOM elements
        const statusEl = document.getElementById('status');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const timerEl = document.getElementById('timer');
        const conversationEl = document.getElementById('conversation');
        const messagesEl = document.getElementById('messages');
        const downloadSection = document.getElementById('downloadSection');
        const downloadBtn = document.getElementById('downloadBtn');
        const voiceSelect = document.getElementById('voiceSelect');
        const recognitionLangSelect = document.getElementById('recognitionLang');

        // Voice and language selection
        voiceSelect.addEventListener('change', (e) => {
            selectedVoice = e.target.value;
            debugLog(`Voice changed to: ${selectedVoice}`);
        });

        recognitionLangSelect.addEventListener('change', (e) => {
            selectedRecognitionLang = e.target.value;
            debugLog(`Recognition language changed to: ${selectedRecognitionLang}`);
            if (recognition) {
                recognition.lang = selectedRecognitionLang;
            }
        });

        // Initialize the application
        async function init() {
            try {
                debugLog('Starting initialization...');
                updateStatus('æ­£åœ¨åˆå§‹åŒ–AIå’ŒèªéŸ³ç³»çµ±... Initializing AI and speech systems...', 'loading');

                // Enable audio context on first user interaction
                document.addEventListener('click', enableAudioContext, { once: true });

                // Initialize AI model first
                debugLog('Initializing AI model...');
                await initializeAI();
                debugLog('AI model initialized successfully');

                // Test Azure connection
                debugLog('Testing Azure connection...');
                updateStatus('æ­£åœ¨æ¸¬è©¦Netlifyé€£æ¥... Testing Netlify function connection...', 'loading');
                await testAzureConnection();
                debugLog('Azure connection test successful');

                // Initialize speech recognition
                updateStatus('æ­£åœ¨è¼‰å…¥èªéŸ³è­˜åˆ¥... Loading speech recognition...', 'loading');
                debugLog('Initializing speech recognition...');
                await initSpeechRecognition();
                debugLog('Speech recognition initialized successfully');

                updateStatus('ğŸ‰ ç³»çµ±æº–å‚™å°±ç·’ï¼é»æ“Š"æ¸¬è©¦éŸ³é »"ç¢ºèªè²éŸ³æ­£å¸¸ï¼System ready!', 'ready');
                startBtn.disabled = false;
                debugLog('Initialization completed successfully');

            } catch (error) {
                console.error('Initialization error:', error);
                debugLog(`Initialization failed: ${error.message}`, 'error');
                updateStatus(`âš ï¸ åˆå§‹åŒ–å¤±æ•— Initialization failed: ${error.message}`, 'error');
            }
        }

        // Initialize speech recognition with compatibility check
        async function initSpeechRecognition() {
            if ('webkitSpeechRecognition' in window) {
                recognition = new webkitSpeechRecognition();
                debugLog('Using webkitSpeechRecognition');
            } else if ('SpeechRecognition' in window) {
                recognition = new SpeechRecognition();
                debugLog('Using SpeechRecognition');
            } else {
                throw new Error('Speech recognition not supported in this browser. Please use Chrome or Edge.');
            }

            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = selectedRecognitionLang;

            recognition.onresult = function (event) {
                const result = event.results[event.results.length - 1];
                if (result.isFinal) {
                    const transcript = result[0].transcript.trim();
                    debugLog(`Speech recognized: ${transcript}`);
                    addMessage('student', transcript);
                    conversationHistory.push({ role: 'student', content: transcript });
                    processStudentInput(transcript);
                }
            };

            recognition.onerror = function (event) {
                debugLog(`Speech recognition error: ${event.error}`, 'error');
                if (event.error === 'no-speech' && isRecording) {
                    setTimeout(() => {
                        try {
                            recognition.start();
                            debugLog('Speech recognition restarted after no-speech');
                        } catch (e) {
                            debugLog(`Recognition restart failed: ${e.message}`, 'error');
                        }
                    }, 1000);
                } else if (event.error === 'not-allowed') {
                    alert('éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•ã€‚è«‹åœ¨ç€è¦½å™¨è¨­ç½®ä¸­å…è¨±éº¥å…‹é¢¨è¨ªå•ã€‚Microphone access denied. Please allow microphone access in browser settings.');
                }
            };

            recognition.onend = function () {
                debugLog('Speech recognition ended');
                if (isRecording) {
                    try {
                        recognition.start();
                        debugLog('Speech recognition restarted');
                    } catch (e) {
                        debugLog(`Recognition restart failed: ${e.message}`, 'error');
                    }
                }
            };

            debugLog('Speech recognition configured successfully');
        }

        // Process student input with AI
        async function processStudentInput(transcript) {
            debugLog(`Processing student input: ${transcript}`);

            try {
                // Create conversation context for AI
                const context = createPatientContext(transcript, conversationHistory);

                // Generate intelligent response using the rule-based AI
                let response;
                if (isAIReady) {
                    const aiResponse = await textGenerator.generateResponse(context, {
                        max_length: 100,
                        temperature: 0.7
                    });

                    const generatedText = aiResponse[0].generated_text;
                    debugLog(`AI generated response: ${generatedText}`);
                    response = formatResponseAsSSML(generatedText);
                } else {
                    throw new Error('AI not ready');
                }

                // Simulate thinking time
                setTimeout(async () => {
                    const responseText = extractTextFromSSML(response);
                    debugLog(`Patient responding: ${responseText}`);

                    // Generate contextual notes
                    const notes = generateContextualNotes(transcript, conversationHistory);

                    addMessage('patient', responseText, notes);
                    conversationHistory.push({ role: 'patient', content: responseText });

                    // Speak the response
                    await speakPatientResponseAzure(response);
                }, 1500 + Math.random() * 1000);

            } catch (error) {
                debugLog(`AI generation failed, using fallback: ${error.message}`, 'error');

                // Use fallback response
                const fallbackResponse = generateFallbackResponse(transcript);
                const responseText = extractTextFromSSML(fallbackResponse);
                addMessage('patient', responseText, "Using fallback response system");
                conversationHistory.push({ role: 'patient', content: responseText });

                setTimeout(async () => {
                    await speakPatientResponseAzure(fallbackResponse);
                }, 1500 + Math.random() * 1000);
            }
        }
        // Generate contextual notes based on conversation flow
        function generateContextualNotes(studentInput, history) {
            const input = studentInput.toLowerCase();
            const conversationLength = history.length;

            if (conversationLength <= 2) {
                if (input.includes('è­·å£«') || input.includes('ä½•') || input.includes('å…¥åšŸ')) {
                    return "Student introduction phase - establishing professional relationship";
                }
            } else if (conversationLength <= 6) {
                if (input.includes('æ–‡') || input.includes('å°å') || input.includes('23332099')) {
                    return "Patient identification verification - important safety procedure";
                }
            } else if (conversationLength <= 10) {
                if (input.includes('æ´—') || input.includes('æ›è—¥') || input.includes('å‚·å£')) {
                    return "Procedure explanation phase - building patient confidence";
                }
            }

            // Advanced contextual notes
            if (input.includes('ç—›') || input.includes('pain')) {
                return "Patient expressing pain concerns - opportunity for empathy and reassurance";
            } else if (input.includes('é©š') || input.includes('scared') || input.includes('afraid')) {
                return "Patient showing anxiety - requires calming approach";
            } else if (input.includes('å””æ˜') || input.includes('understand')) {
                return "Patient needs clarification - good opportunity to demonstrate clear communication";
            }

            return "Ongoing conversation - maintain professional rapport";
        }

        // Extract text from SSML
        function extractTextFromSSML(ssml) {
            const tempDiv = document.createElement('div');
            tempDiv.innerHTML = ssml.replace(/<[^>]*>/g, ' ').replace(/\s+/g, ' ').trim();
            return tempDiv.textContent;
        }

        // Enhanced Azure TTS with better audio handling
        async function speakPatientResponseAzure(ssmlText, isTest = false) {
            try {
                debugLog('Sending TTS request to Azure via Netlify function');
                debugLog(`SSML length: ${ssmlText.length} characters`);

                const response = await fetch(CURRENT_ENDPOINT, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ ssml: ssmlText })
                });

                debugLog(`TTS response status: ${response.status}`);

                if (response.ok) {
                    const data = await response.json();
                    debugLog('TTS response received successfully');
                    debugLog(`Audio data length: ${data.audio ? data.audio.length : 'undefined'} characters`);

                    // Enhanced audio playing with better browser compatibility
                    const audioBlob = new Blob(
                        [Uint8Array.from(atob(data.audio), c => c.charCodeAt(0))],
                        { type: 'audio/mpeg' }
                    );
                    const audioUrl = URL.createObjectURL(audioBlob);

                    // Create and configure audio element
                    const audio = new Audio(audioUrl);
                    audio.volume = currentVolume;
                    audio.preload = 'auto';

                    // Enhanced audio event handlers
                    audio.oncanplay = () => {
                        debugLog('Audio ready to play');
                        if (isTest) {
                            updateAudioStatus('ğŸµ éŸ³é »å·²åŠ è¼‰ï¼Œæº–å‚™æ’­æ”¾... Audio loaded, ready to play...', 'warning');
                        }
                    };

                    audio.onplay = () => {
                        debugLog('Audio playback started');
                        if (isTest) {
                            updateAudioStatus('â–¶ï¸ éŸ³é »æ­£åœ¨æ’­æ”¾... Audio playing...', 'warning');
                        }
                    };

                    audio.onended = () => {
                        debugLog('Audio playback ended');
                        URL.revokeObjectURL(audioUrl);
                        if (isTest) {
                            updateAudioStatus('âœ… éŸ³é »æ’­æ”¾å®Œæˆï¼Audio playback completed!', 'success');
                        }
                    };

                    audio.onerror = (e) => {
                        debugLog(`Audio playback error: ${e.message || 'Unknown error'}`, 'error');
                        if (isTest) {
                            updateAudioStatus('âŒ éŸ³é »æ’­æ”¾å¤±æ•— Audio playback failed', 'error');
                        }
                    };

                    // Multiple play attempts for better browser compatibility
                    const playAudio = async () => {
                        try {
                            // Ensure audio context is active
                            if (audioContext && audioContext.state === 'suspended') {
                                await audioContext.resume();
                                debugLog('Audio context resumed before playback');
                            }

                            // First attempt
                            await audio.play();
                            debugLog('Audio playback started successfully');

                        } catch (firstError) {
                            debugLog(`First play attempt failed: ${firstError.message}`, 'error');

                            // Second attempt with user interaction
                            try {
                                // Wait a bit and try again
                                await new Promise(resolve => setTimeout(resolve, 100));
                                await audio.play();
                                debugLog('Audio playback started on second attempt');

                            } catch (secondError) {
                                debugLog(`Second play attempt failed: ${secondError.message}`, 'error');

                                // Show user instruction for manual play
                                if (isTest) {
                                    updateAudioStatus('âš ï¸ éœ€è¦ç”¨æˆ¶äº¤äº’æ‰èƒ½æ’­æ”¾éŸ³é »ã€‚è«‹é»æ“Šé é¢ä»»æ„ä½ç½®å¾Œé‡è©¦ã€‚', 'warning');
                                }

                                // Try to play on next user interaction
                                const playOnInteraction = () => {
                                    audio.play().then(() => {
                                        debugLog('Audio played after user interaction');
                                        document.removeEventListener('click', playOnInteraction);
                                    }).catch(e => {
                                        debugLog(`Play after interaction failed: ${e.message}`, 'error');
                                    });
                                };

                                document.addEventListener('click', playOnInteraction, { once: true });
                            }
                        }
                    };

                    // Start playback
                    await playAudio();

                } else {
                    const errorData = await response.json();
                    debugLog(`TTS function failed: ${response.status}`, 'error');
                    debugLog(`Error details: ${JSON.stringify(errorData)}`, 'error');

                    if (isTest) {
                        updateAudioStatus(`âŒ TTS è«‹æ±‚å¤±æ•—: ${response.status}`, 'error');
                    }
                }
            } catch (error) {
                debugLog(`TTS function error: ${error.message}`, 'error');
                console.error('Speech function error:', error);

                if (isTest) {
                    updateAudioStatus(`âŒ éŸ³é »è«‹æ±‚éŒ¯èª¤: ${error.message}`, 'error');
                }
            }
        }

        // Add message to conversation
        function addMessage(speaker, text, notes = null) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${speaker}-message`;

            const header = document.createElement('div');
            header.className = 'message-header';
            header.innerHTML = speaker === 'student' ?
                'ğŸ‘©â€âš•ï¸ è­·ç†å­¸ç”Ÿä½•åŒå­¸ Student Ho' :
                'ğŸ‘¨ ç—…äººæ–‡å…ˆç”Ÿ Patient Mr. Man';

            const content = document.createElement('div');
            content.className = 'message-content';
            content.innerHTML = `<span class="cantonese-text">${text}</span>`;

            messageDiv.appendChild(header);
            messageDiv.appendChild(content);

            if (notes && speaker === 'patient') {
                const notesDiv = document.createElement('div');
                notesDiv.className = 'procedure-notes';
                notesDiv.textContent = `ğŸ“ ${notes}`;
                messageDiv.appendChild(notesDiv);
            }

            messagesEl.appendChild(messageDiv);
            messagesEl.scrollTop = messagesEl.scrollHeight;
        }

        // Start training session with audio check
        async function startSession() {
            try {
                debugLog('Starting training session');

                // Check if AI is ready
                if (!isAIReady) {
                    alert('AIæ¨¡å‹å°šæœªå°±ç·’ï¼Œè«‹ç¨ç­‰ç‰‡åˆ»å†è©¦ï¼\nAI model not ready yet, please wait and try again!');
                    return;
                }

                // Check audio readiness first
                if (!audioContext || audioContext.state === 'suspended') {
                    alert('è«‹å…ˆé»æ“Š"å•Ÿç”¨éŸ³é »"æŒ‰éˆ•ï¼Œç¢ºä¿éŸ³é »æ­£å¸¸å·¥ä½œï¼\nPlease click "Enable Audio" button first to ensure audio works properly!');
                    return;
                }

                isRecording = true;
                audioChunks = [];
                conversationHistory = [];

                // Clear previous conversation
                messagesEl.innerHTML = '';
                conversationEl.style.display = 'block';

                // Start audio recording
                debugLog('Requesting microphone access');
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                debugLog('Microphone access granted');

                mediaRecorder = new MediaRecorder(stream, {
                    mimeType: 'audio/webm;codecs=opus'
                });

                mediaRecorder.ondataavailable = function (event) {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = function () {
                    debugLog('Media recorder stopped, creating download link');
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    createDownloadLink(audioBlob);
                };

                mediaRecorder.start();
                debugLog('Audio recording started');

                // Start speech recognition
                debugLog('Starting speech recognition');
                recognition.start();

                // Start timer
                sessionStartTime = Date.now();
                startTimer();

                // Initial scenario setup with AI-enhanced greeting
                setTimeout(async () => {
                    debugLog('Playing initial patient greeting');

                    // Use enhanced rule-based AI to generate initial greeting
                    let initialResponse;
                    try {
                        if (isAIReady) {
                            const context = createPatientContext("æœ‰äººé€²å…¥æˆ¿é–“", []);
                            const aiGreeting = await textGenerator.generateResponse(context, {
                                max_length: 50,
                                temperature: 0.7
                            });

                            let generatedGreeting = aiGreeting[0].generated_text;
                            if (generatedGreeting.length < 10) {
                                generatedGreeting = "å‘€ï¼Ÿæœ‰äººåšŸå–‡ã€‚é‚Šå€‹å‘€ï¼ŸWho is there?";
                            }

                            initialResponse = formatResponseAsSSML(generatedGreeting);
                            debugLog(`AI-generated initial greeting: ${generatedGreeting}`);
                        } else {
                            throw new Error('AI not ready');
                        }
                    } catch (error) {
                        debugLog(`Using fallback greeting: ${error.message}`);
                        initialResponse = `<speak version="1.0" xml:lang="zh-HK">
                            <voice name="${selectedVoice}">
                                <mstts:express-as style="curious">
                                    å‘€ï¼Ÿæœ‰äººåšŸå–‡ã€‚é‚Šå€‹å‘€ï¼Ÿ
                                    <lang xml:lang="en-US">Who is there?</lang>
                                </mstts:express-as>
                            </voice>
                        </speak>`;
                    }

                    const responseText = extractTextFromSSML(initialResponse);
                    addMessage('patient', responseText, "Patient notices someone approaching the room - training session begins");
                    conversationHistory.push({ role: 'patient', content: responseText });
                    await speakPatientResponseAzure(initialResponse);
                }, 3000);

                updateStatus('ğŸ™ï¸ æ™ºèƒ½AIè¨“ç·´é€²è¡Œä¸­... Intelligent AI training in progress', 'recording');
                startBtn.disabled = true;
                stopBtn.disabled = false;
                debugLog('Training session started successfully');

            } catch (error) {
                debugLog(`Error starting session: ${error.message}`, 'error');
                console.error('Error starting session:', error);

                if (error.name === 'NotAllowedError') {
                    alert('éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•ã€‚è«‹åœ¨ç€è¦½å™¨è¨­ç½®ä¸­å…è¨±éº¥å…‹é¢¨è¨ªå•ã€‚Microphone access denied. Please allow microphone access in browser settings.');
                } else {
                    alert('ç„¡æ³•ä½¿ç”¨éº¥å…‹é¢¨ï¼Œè«‹æª¢æŸ¥æ¬Šé™è¨­å®š Error accessing microphone. Please check permissions.');
                }
            }
        }

        // Stop training session
        function stopSession() {
            debugLog('Stopping training session');
            isRecording = false;

            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                debugLog('Media recorder stopped');
            }

            if (recognition) {
                recognition.stop();
                debugLog('Speech recognition stopped');
            }

            stopTimer();

            updateStatus('âœ… æ™ºèƒ½è¨“ç·´å®Œæˆï¼Intelligent training completed! Review your session below.', 'ready');
            startBtn.disabled = false;
            stopBtn.disabled = true;

            downloadSection.style.display = 'block';
            debugLog('Training session stopped successfully');
        }

        // Timer functions
        function startTimer() {
            timerInterval = setInterval(() => {
                const elapsed = Date.now() - sessionStartTime;
                const minutes = Math.floor(elapsed / 60000);
                const seconds = Math.floor((elapsed % 60000) / 1000);
                timerEl.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
            }, 1000);
        }

        function stopTimer() {
            if (timerInterval) {
                clearInterval(timerInterval);
            }
        }

        // Create download link for recording
        function createDownloadLink(audioBlob) {
            const url = URL.createObjectURL(audioBlob);
            downloadBtn.onclick = function () {
                const a = document.createElement('a');
                a.href = url;
                a.download = `ai-nursing-training-${new Date().toISOString().slice(0, 19)}.webm`;
                a.click();
            };
        }

        // Update status display
        function updateStatus(message, type) {
            statusEl.textContent = message;
            statusEl.className = `status ${type}`;
        }

        // Make functions available globally
        window.testAudio = testAudio;
        window.enableAudioContext = enableAudioContext;
        window.clearDebugLogs = clearDebugLogs;
        window.toggleDebug = toggleDebug;

        // Event listeners
        startBtn.addEventListener('click', startSession);
        stopBtn.addEventListener('click', stopSession);

        // Auto-enable audio on any user interaction
        const enableAudioOnInteraction = () => {
            enableAudioContext();
            document.removeEventListener('click', enableAudioOnInteraction);
            document.removeEventListener('keydown', enableAudioOnInteraction);
            document.removeEventListener('touchstart', enableAudioOnInteraction);
        };

        document.addEventListener('click', enableAudioOnInteraction);
        document.addEventListener('keydown', enableAudioOnInteraction);
        document.addEventListener('touchstart', enableAudioOnInteraction);

        // Initialize on page load
        init();
    </script>
</body>

</html>