</html><!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è­·ç†å­¸ç”Ÿå‚·å£è­·ç†è¨“ç·´ - Nursing Wound Care Training (Audio Files Version)</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }

        .main-container {
            max-width: 1400px;
            margin: 0 auto;
            display: grid;
            grid-template-columns: 1fr 350px;
            gap: 20px;
        }

        .left-panel {
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 40px rgba(0, 0, 0, 0.1);
        }

        .right-panel {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }

        .info-card,
        .settings-card {
            background: white;
            border-radius: 15px;
            padding: 20px;
            box-shadow: 0 10px 30px rgba(0, 0, 0, 0.1);
        }

        .header {
            text-align: center;
            margin-bottom: 30px;
        }

        .header h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 28px;
        }

        .header h2 {
            color: #34495e;
            margin-bottom: 15px;
            font-size: 18px;
        }

        .audio-files-badge {
            background: #e74c3c;
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
            margin-left: 10px;
        }

        .ai-badge {
            background: #28a745;
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
            margin-left: 10px;
        }

        .scenario-info {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }

        .patient-info {
            background: #e8f5e8;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #27ae60;
        }

        .patient-info h4 {
            margin-top: 0;
            color: #155724;
        }

        .controls {
            display: flex;
            gap: 15px;
            justify-content: center;
            margin: 25px 0;
            flex-wrap: wrap;
        }

        .btn {
            padding: 12px 24px;
            border: none;
            border-radius: 8px;
            font-size: 16px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: 600;
        }

        .btn-primary {
            background: #3498db;
            color: white;
        }

        .btn-primary:hover {
            background: #2980b9;
            transform: translateY(-2px);
        }

        .btn-success {
            background: #28a745;
            color: white;
        }

        .btn-success:hover {
            background: #218838;
        }

        .btn-danger {
            background: #e74c3c;
            color: white;
        }

        .btn-danger:hover {
            background: #c0392b;
        }

        .btn:disabled {
            background: #bdc3c7;
            cursor: not-allowed;
            transform: none;
        }

        .btn-small {
            padding: 8px 16px;
            font-size: 14px;
        }

        .status {
            text-align: center;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            font-weight: 600;
        }

        .status.loading {
            background: #fff3cd;
            color: #856404;
        }

        .status.ready {
            background: #d4edda;
            color: #155724;
        }

        .status.recording {
            background: #f8d7da;
            color: #721c24;
            animation: pulse 1.5s infinite;
        }

        .status.error {
            background: #f8d7da;
            color: #721c24;
        }

        @keyframes pulse {

            0%,
            100% {
                opacity: 1;
            }

            50% {
                opacity: 0.7;
            }
        }

        .timer {
            font-size: 24px;
            font-weight: bold;
            text-align: center;
            color: #2c3e50;
            margin: 20px 0;
        }

        .conversation {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            max-height: 400px;
            overflow-y: auto;
        }

        .message {
            margin: 15px 0;
            padding: 15px;
            border-radius: 8px;
            line-height: 1.5;
        }

        .student-message {
            background: #e3f2fd;
            margin-left: 20px;
            border-left: 4px solid #2196f3;
        }

        .patient-message {
            background: #fff3e0;
            margin-right: 20px;
            border-left: 4px solid #ff9800;
        }

        .message-header {
            font-weight: bold;
            margin-bottom: 8px;
            color: #2c3e50;
        }

        .message-content {
            font-size: 16px;
        }

        .cantonese-text {
            color: #c0392b;
            font-weight: 500;
        }

        .procedure-notes {
            background: #f0f8ff;
            padding: 10px;
            margin: 10px 0;
            border-radius: 6px;
            font-style: italic;
            color: #34495e;
            border-left: 3px solid #3498db;
        }

        .download-section {
            text-align: center;
            margin-top: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
        }

        .loading-spinner {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid #f3f3f3;
            border-top: 3px solid #3498db;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            0% {
                transform: rotate(0deg);
            }

            100% {
                transform: rotate(360deg);
            }
        }

        .settings-card h4 {
            margin-top: 0;
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }

        .setting-group {
            margin: 15px 0;
        }

        .setting-group label {
            display: block;
            margin-bottom: 5px;
            font-weight: 600;
            color: #34495e;
        }

        .setting-group select {
            width: 100%;
            padding: 8px;
            border-radius: 6px;
            border: 1px solid #ddd;
            font-size: 14px;
        }

        .audio-status {
            padding: 10px;
            border-radius: 6px;
            margin: 10px 0;
            font-weight: bold;
            text-align: center;
        }

        .audio-status.success {
            background: #d4edda;
            color: #155724;
        }

        .audio-status.warning {
            background: #fff3cd;
            color: #856404;
        }

        .audio-status.error {
            background: #f8d7da;
            color: #721c24;
        }

        .volume-control {
            margin: 10px 0;
        }

        .volume-control input[type="range"] {
            width: 100%;
            margin: 5px 0;
        }

        .debug-section {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #dc3545;
            font-family: monospace;
            font-size: 12px;
            max-height: 200px;
            overflow-y: auto;
        }

        .debug-section h4 {
            color: #dc3545;
            margin-top: 0;
        }

        .debug-log {
            background: #f8f9fa;
            padding: 5px;
            margin: 5px 0;
            border-radius: 4px;
            color: #6c757d;
        }

        .debug-error {
            background: #f8d7da;
            color: #721c24;
        }

        .debug-info {
            background: #d1ecf1;
            color: #0c5460;
        }

        .ai-status {
            background: #e8f5e8;
            padding: 10px;
            border-radius: 6px;
            margin: 10px 0;
            font-weight: bold;
            text-align: center;
            border-left: 4px solid #28a745;
        }

        .ai-status.loading {
            background: #fff3cd;
            color: #856404;
            border-left-color: #ffc107;
        }

        .ai-status.ready {
            background: #d4edda;
            color: #155724;
            border-left-color: #28a745;
        }

        .ai-status.error {
            background: #f8d7da;
            color: #721c24;
            border-left-color: #dc3545;
        }

        .audio-files-status {
            background: #fff3e0;
            padding: 10px;
            border-radius: 6px;
            margin: 10px 0;
            font-weight: bold;
            text-align: center;
            border-left: 4px solid #ff9800;
        }

        @media (max-width: 1200px) {
            .main-container {
                grid-template-columns: 1fr;
                max-width: 900px;
            }

            .right-panel {
                flex-direction: row;
                flex-wrap: wrap;
            }

            .info-card,
            .settings-card {
                flex: 1;
                min-width: 300px;
            }
        }

        @media (max-width: 768px) {
            .right-panel {
                flex-direction: column;
            }

            .controls {
                flex-direction: column;
                align-items: center;
            }

            .btn {
                width: 100%;
                max-width: 300px;
            }
        }
    </style>
</head>

<body>
    <div class="main-container">
        <div class="left-panel">
            <div class="header">
                <h1>ğŸ¥ è­·ç†å­¸ç”Ÿå‚·å£è­·ç†è¨“ç·´ç³»çµ± <span class="audio-files-badge">Audio Files</span><span
                        class="ai-badge">Transformer.js</span></h1>
                <h2>Professional Nursing Student Wound Care Training System</h2>
                <p>Advanced AI-powered patient simulation with pre-recorded realistic Cantonese voices</p>
            </div>

            <div class="scenario-info">
                <h3>ğŸ“‹ è¨“ç·´å ´æ™¯ Training Scenario</h3>
                <p><strong>ä½ çš„è§’è‰² Your Role:</strong> XXå¤§å­¸è­·ç†å­¸ç”Ÿä½•åŒå­¸ (Nursing Student Ho from XX University)</p>
                <p><strong>ä»»å‹™ Task:</strong> é€²è¡Œå‚·å£è­·ç†ç¨‹åºä¸¦èˆ‡ç—…äººä¿æŒè‰¯å¥½æºé€š</p>
                <p><strong>é‡é» Focus:</strong> èº«ä»½ç¢ºèªã€ç¨‹åºè§£é‡‹ã€ç—…äººå®‰æ’«ã€å°ˆæ¥­æºé€š</p>
            </div>

            <div id="aiStatus" class="ai-status loading">
                <span class="loading-spinner"></span> æ­£åœ¨åŠ è¼‰AIæ¨¡å‹å’ŒéŸ³é »æ–‡ä»¶... Loading AI model and audio files...
            </div>

            <div id="audioFilesStatus" class="audio-files-status">
                ğŸµ ç­‰å¾…éŸ³é »æ–‡ä»¶ç³»çµ±åˆå§‹åŒ–... Waiting for audio files system initialization...
            </div>

            <div id="status" class="status loading">
                <span class="loading-spinner"></span> æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½åŒ¹é…ç³»çµ±... Initializing intelligent audio matching system...
            </div>

            <div class="controls">
                <button id="startBtn" class="btn btn-primary" disabled>ğŸ™ï¸ é–‹å§‹è¨“ç·´ Start Training</button>
                <button id="stopBtn" class="btn btn-danger" disabled>â¹ï¸ çµæŸè¨“ç·´ Stop Training</button>
            </div>

            <div id="timer" class="timer">00:00</div>

            <div id="conversation" class="conversation" style="display: none;">
                <h4>ğŸ’¬ å°è©±è¨˜éŒ„ Session Conversation:</h4>
                <div id="messages"></div>
            </div>

            <div id="downloadSection" class="download-section" style="display: none;">
                <h4>ğŸ“¥ è¨“ç·´è¨˜éŒ„ Session Recording</h4>
                <p>ä½ çš„è¨“ç·´å°è©±å·²è¢«è¨˜éŒ„ä»¥ä¾›åˆ†æ Your training session has been recorded for analysis.</p>
                <button id="downloadBtn" class="btn btn-primary">ä¸‹è¼‰éŒ„éŸ³ Download Recording</button>
            </div>

            <div id="debugSection" class="debug-section" style="display: none;">
                <h4>ğŸ”§ Debug Information</h4>
                <div id="debugLogs"></div>
                <div style="margin-top: 10px;">
                    <button onclick="clearDebugLogs()" class="btn btn-primary btn-small">Clear Logs</button>
                    <button onclick="toggleDebug()" class="btn btn-primary btn-small">Hide Debug</button>
                </div>
            </div>

            <div class="controls">
                <button onclick="toggleDebug()" class="btn btn-primary btn-small">ğŸ”§ Toggle Debug</button>
            </div>
        </div>

        <div class="right-panel">
            <div class="info-card">
                <h4>ğŸ‘¨ ç—…äººè³‡æ–™ Patient Information</h4>
                <div class="patient-info">
                    <p><strong>å§“å Name:</strong> é™ˆå°ä¸½å¥³å£« (Ms. Chan Siu Lai)</p>
                    <p><strong>å¹´é½¡ Age:</strong> 65æ­²</p>
                    <p><strong>èªè¨€ Language:</strong> å»£æ±è©±/è‹±æ–‡ (Cantonese/English Mix)</p>
                    <p><strong>ç—…æƒ… Condition:</strong> è…¿éƒ¨å‚·å£éœ€è¦æ¸…æ´—åŠæ›è—¥ (Leg wound requiring cleaning and dressing)</p>
                    <p><strong>æ€§æ ¼ç‰¹é»:</strong> å¥è«‡ä½†å®¹æ˜“ç·Šå¼µï¼Œæœƒé…åˆæ²»ç™‚ä½†æœ‰ç–¼ç—›åæ‡‰</p>
                </div>
            </div>

            <div class="settings-card">
                <h4>ğŸµ éŸ³é »æ–‡ä»¶è¨­å®š Audio Files Settings</h4>

                <div class="setting-group">
                    <label>Audio Response Mode:</label>
                    <select id="responseMode">
                        <option value="intelligent">æ™ºèƒ½åŒ¹é… Intelligent Matching</option>
                        <option value="sequential">é †åºæ’­æ”¾ Sequential Play</option>
                        <option value="random">éš¨æ©Ÿé¸æ“‡ Random Selection</option>
                    </select>
                </div>

                <div class="setting-group">
                    <label>Speech Recognition:</label>
                    <select id="recognitionLang">
                        <option value="zh-HK">å»£æ±è©± Cantonese (Hong Kong)</option>
                        <option value="en-US">English (US)</option>
                        <option value="zh-CN">æ™®é€šè©± Mandarin</option>
                    </select>
                </div>

                <h4>ğŸ”Š éŸ³é »æ¸¬è©¦ Audio Test</h4>

                <div id="audioStatus" class="audio-status warning">
                    ç­‰å¾…éŸ³é »æ¸¬è©¦... Waiting for audio test...
                </div>

                <div class="volume-control">
                    <label for="volumeSlider">éŸ³é‡ Volume:</label>
                    <input type="range" id="volumeSlider" min="0" max="1" step="0.1" value="0.8">
                    <div style="text-align: center; margin-top: 5px;">
                        <span id="volumeDisplay">80%</span>
                    </div>
                </div>

                <div style="display: flex; flex-direction: column; gap: 10px; margin-top: 15px;">
                    <button onclick="testAudio()" class="btn btn-success btn-small">ğŸµ æ¸¬è©¦éŸ³é » Test Audio</button>
                    <button onclick="enableAudioContext()" class="btn btn-primary btn-small">ğŸ”“ å•Ÿç”¨éŸ³é » Enable
                        Audio</button>
                </div>

                <div id="audioFilesInfo" class="setting-group" style="margin-top: 15px;">
                    <label>Available Audio Files:</label>
                    <div id="audioFilesList" style="font-size: 12px; color: #666; max-height: 100px; overflow-y: auto;">
                        Loading...
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script type="module">
        import { pipeline, env } from 'https://cdn.jsdelivr.net/npm/@xenova/transformers@2.17.2';

        // Disable local model loading to use CDN
        env.allowRemoteModels = true;
        env.allowLocalModels = false;

        // Audio Files System Configuration
        let audioMetadata = {};
        let audioFilesLoaded = false;
        let audioFilesList = [];
        let currentVolume = 0.8;

        // Global variables
        let mediaRecorder;
        let audioChunks = [];
        let recognition;
        let sessionStartTime;
        let timerInterval;
        let isRecording = false;
        let conversationHistory = [];
        let selectedRecognitionLang = 'zh-HK';
        let selectedResponseMode = 'intelligent';
        let debugMode = false;
        let audioContext = null;
        let conversationFailureCount = 0;
        let lastFailedPhase = null;

        // AI Model variables
        let textGenerator = null;
        let isAIReady = false;

        // Patient personality and context
        const patientContext = {
            name: "é™ˆå°ä¸½",
            age: 65,
            condition: "è…¿éƒ¨å‚·å£éœ€è¦æ¸…æ´—åŠæ›è—¥",
            personality: "å¥è«‡ä½†å®¹æ˜“ç·Šå¼µï¼Œæœƒé…åˆæ²»ç™‚ä½†æœ‰ç–¼ç—›åæ‡‰ï¼Œæœƒæ··åˆå»£æ±è©±èªªè©±",
            currentPain: 6,
            anxiety: 7,
            previousExperience: "ä¹‹å‰æœ‰åšéå‚·å£è­·ç†ï¼Œä½†æ¯æ¬¡éƒ½å¾ˆç·Šå¼µ"
        };

        // Load audio metadata from the provided JSON
        async function loadAudioMetadata() {
            try {
                debugLog('Loading audio metadata...');
                updateAudioFilesStatus('ğŸ”„ æ­£åœ¨è¼‰å…¥éŸ³é »å…ƒæ•¸æ“š... Loading audio metadata...');

                // In a real implementation, you would fetch this from a server
                // For now, we'll use the provided metadata directly
                audioMetadata = {
                                "patient_greet_painful.m4a": {
                    "filename": "patient_greet_painful.m4a",
                    "text": "å¥½å•Šï¼Œä½ å…¥é»å•¦ï¼Œæˆ‘çœŸç³»å¥½ç—›å•Šï¼",
                    "duration": 5.0,
                    "conversation_phase": "åˆæ¬¡æ¥è§¦",
                    "emotions": {
                        "primary": "ç–¼ç—›",
                        "intensity": 3,
                        "secondary": ["ç´§å¼ ", "å¯»æ±‚å¸®åŠ©"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["é™Œç”Ÿäººè¿›å…¥", "æ„Ÿåˆ°ç–¼ç—›"],
                        "incompatible_scenarios": ["è¡¨è¾¾æ„Ÿè°¢"]
                    }
                },
                "patient_helpmecleanse.m4a": {
                    "filename": "patient_helpmecleanse.m4a",
                    "text": "å¥½å•Šï¼Œä½ å¸®æˆ‘æ´—å•Šã€‚",
                    "duration": 2.0,
                    "conversation_phase": "ç¨‹åºè§£é‡Š",
                    "emotions": {
                        "primary": "é…åˆ",
                        "intensity": 1,
                        "secondary": ["é…åˆ"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["å¼€å§‹æ“ä½œ"],
                        "incompatible_scenarios": ["æ„Ÿåˆ°ç–¼ç—›"]
                    }
                },
                "patient_first_meeting_procedure_okay.m4a": {
                    "filename": "patient_first_meeting_procedure_okay.m4a",
                    "text": "å¥½å•Š",
                    "duration": 1.0,
                    "conversation_phase": "åˆæ¬¡æ¥è§¦",
                    "emotions": {
                        "primary": "é…åˆ",
                        "intensity": 1,
                        "secondary": ["é…åˆ"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["é™Œç”Ÿäººè¿›å…¥", "ç¨‹åºè§£é‡Šä¸­"],
                        "incompatible_scenarios": ["æ„Ÿåˆ°ç–¼ç—›"]
                    }
                },
                "patient_I'mChenXiaoLi.m4a": {
                    "filename": "patient_I'mChenXiaoLi.m4a",
                    "text": "æˆ‘å«é™ˆå°ä¸½ã€‚",
                    "duration": 2.0,
                    "conversation_phase": "èº«ä»½ç¡®è®¤",
                    "emotions": {
                        "primary": "é…åˆ",
                        "intensity": 1,
                        "secondary": ["å¯»æ±‚å¸®åŠ©"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["éœ€è¦èº«ä»½ç¡®è®¤", "å…³å¿ƒåç»­"],
                        "incompatible_scenarios": ["æ“ä½œè¿›è¡Œä¸­"]
                    }
                },
                "patient_painful_abit.m4a": {
                    "filename": "patient_painful_abit.m4a",
                    "text": "æœ‰å°‘å°‘ç—›å•Šã€‚",
                    "duration": 1.0,
                    "conversation_phase": "ç¨‹åºè¿›è¡Œ",
                    "emotions": {
                        "primary": "ç–¼ç—›",
                        "intensity": 1,
                        "secondary": ["é…åˆ", "å¯»æ±‚å¸®åŠ©"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["æ“ä½œè¿›è¡Œä¸­", "æ„Ÿåˆ°ç–¼ç—›"],
                        "incompatible_scenarios": ["è¡¨è¾¾æ„Ÿè°¢"]
                    }
                },
                "patient_anxious_painful.m4a": {
                    "filename": "patient_anxious_painful.m4a",
                    "text": "å“å‘€ï¼Œæˆ‘æœ‰å°‘å°‘ç—›å•Šï¼",
                    "duration": 6.0,
                    "conversation_phase": "ç¨‹åºè¿›è¡Œ",
                    "emotions": {
                        "primary": "ç–¼ç—›",
                        "intensity": 3,
                        "secondary": ["é…åˆ"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["æ“ä½œè¿›è¡Œä¸­", "æ„Ÿåˆ°ç–¼ç—›"],
                        "incompatible_scenarios": ["è¡¨è¾¾æ„Ÿè°¢"]
                    }
                },
                "patient_rudeswearing_painful.m4a": {
                    "filename": "patient_rudeswearing_painful.m4a",
                    "text": "å±Œï¼Œå’é–ªç—›å™¶ï¼",
                    "duration": 2.0,
                    "conversation_phase": "ç¨‹åºè¿›è¡Œ",
                    "emotions": {
                        "primary": "ç–¼ç—›",
                        "intensity": 3,
                        "secondary": ["ç´§å¼ ", "å¯»æ±‚å¸®åŠ©"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["æ“ä½œè¿›è¡Œä¸­", "æ„Ÿåˆ°ç–¼ç—›"],
                        "incompatible_scenarios": ["è¡¨è¾¾æ„Ÿè°¢"]
                    }
                },
                "patient_don't_touch_me.m4a": {
                    "filename": "patient_don't_touch_me.m4a",
                    "text": "å˜©ï¼Œå’ç—›å™¶ï¼Œå””å¥½å†ææˆ‘å•¦ï¼",
                    "duration": 3.0,
                    "conversation_phase": "ç¨‹åºè¿›è¡Œ",
                    "emotions": {
                        "primary": "ç–¼ç—›",
                        "intensity": 3,
                        "secondary": ["ç´§å¼ ", "å¯»æ±‚å¸®åŠ©"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["æ“ä½œè¿›è¡Œä¸­", "æ„Ÿåˆ°ç–¼ç—›"],
                        "incompatible_scenarios": ["è¡¨è¾¾æ„Ÿè°¢"]
                    }
                },
                "patient_angry_to_file_complaints.m4a": {
                    "filename": "patient_angry_to_file_complaints.m4a",
                    "text": "æˆ‘å¾ˆä¸æ»¿æ„ï¼Œæˆ‘è¦å‘ä½ ä¸Šå¸æŠ•è¨´ä½ ï¼ˆè­·å£«ï¼‰ï¼",
                    "duration": 4.0,
                    "conversation_phase": "ç¨‹åºå®Œæˆ",
                    "emotions": {
                        "primary": "æ„¤æ€’",
                        "intensity": 3,
                        "secondary": ["ä¸æ»¡"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["ç¨‹åºå®Œæˆ", "æ“ä½œè¿›è¡Œä¸­"],
                        "incompatible_scenarios": ["è¡¨è¾¾æ„Ÿè°¢"]
                    }
                },
                "patient_nothingspecial_thanks.m4a": {
                    "filename": "patient_nothingspecial_thanks.m4a",
                    "text": "å“¦ï¼Œ okay å–ï¼Œå†‡ä¹ˆç‰¹åˆ«å¾èˆ’æœï¼Œå¾è¯¥æ™’ï¼",
                    "duration": 4.0,
                    "conversation_phase": "ç¨‹åºå®Œæˆ",
                    "emotions": {
                        "primary": "å®‰å¿ƒ",
                        "intensity": 2,
                        "secondary": ["æ„Ÿæ¿€", "é…åˆ"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["ç¨‹åºå®Œæˆ", "æ“ä½œè¿›è¡Œä¸­"],
                        "incompatible_scenarios": ["æ„Ÿåˆ°ç–¼ç—›"]
                    }
                },
                "patient_complaining_the_pain.m4a": {
                    "filename": "patient_complaining_the_pain.m4a",
                    "text": "å“—ï¼Œå§‘å¨˜ï¼Œä½ å¾—å””å¾—è½»æ‰‹Då•Šï¼ŒçœŸç³»å¥½ç—›å•Šï¼",
                    "duration": 5.0,
                    "conversation_phase": "ç¨‹åºè¿›è¡Œ",
                    "emotions": {
                        "primary": "ç–¼ç—›",
                        "intensity": 3,
                        "secondary": ["ç´§å¼ ", "ç–‘é—®"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["æ“ä½œè¿›è¡Œä¸­", "å¼€å§‹æ“ä½œ"],
                        "incompatible_scenarios": ["è¡¨è¾¾æ„Ÿè°¢"]
                    }
                },
                "patient_personal_attack_on_nurses.m4a": {
                    "filename": "patient_personal_attack_on_nurses.m4a",
                    "text": "å“å‘€ï¼Œä¹ˆä½ å’ç²—é²å™¶ï¼Œå¥½å¤§åŠ›å•Šï¼",
                    "duration": 5.0,
                    "conversation_phase": "ç¨‹åºè¿›è¡Œ",
                    "emotions": {
                        "primary": "ç–¼ç—›",
                        "intensity": 3,
                        "secondary": ["ç´§å¼ "]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["æ“ä½œè¿›è¡Œä¸­", "æ„Ÿåˆ°ç–¼ç—›"],
                        "incompatible_scenarios": ["è¡¨è¾¾æ„Ÿè°¢"]
                    }
                },
                "patient_revisit_checkedname.m4a": {
                    "filename": "patient_revisit_checkedname.m4a",
                    "text": "æ˜¯è¿™ä¸ªç¼–å·å’Œå§“å.",
                    "duration": 5.0,
                    "conversation_phase": "ç¨‹åºå®Œæˆ",
                    "emotions": {
                        "primary": "é…åˆ",
                        "intensity": 1,
                        "secondary": []
                    },
                    "context_tags": {
                        "suitable_scenarios": ["å…³å¿ƒåç»­"],
                        "incompatible_scenarios": ["éœ€è¦èº«ä»½ç¡®è®¤"]
                    }
                },
                "patient_discharge.m4a": {
                    "filename": "patient_discharge.m4a",
                    "text": "å“å‘€ï¼Œå“å‘€ï¼Œåšä¹ˆæˆ‘ä¼¤å£é»„è‰²çš„ï¼Ÿæµç€å‡ºæ¥çš„æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "duration": 6.0,
                    "conversation_phase": "ç¨‹åºè¿›è¡Œ",
                    "emotions": {
                        "primary": "ç´§å¼ ",
                        "intensity": 3,
                        "secondary": ["å¥½å¥‡", "å›°æƒ‘"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["æ“ä½œè¿›è¡Œä¸­", "éœ€è¦å®‰æŠš"],
                        "incompatible_scenarios": ["è¡¨è¾¾æ„Ÿè°¢"]
                    }
                },
                "patient_Nothing_unconfortable.m4a": {
                    "filename": "patient_Nothing_unconfortable.m4a",
                    "text": "å†‡å•Šï¼Œå†‡ç‰¹åˆ«å¾èˆ’æœã€‚",
                    "duration": 3.0,
                    "conversation_phase": "ç¨‹åºå®Œæˆ",
                    "emotions": {
                        "primary": "é…åˆ",
                        "intensity": 1,
                        "secondary": ["é…åˆ", "å®‰å¿ƒ"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["å…³å¿ƒåç»­"],
                        "incompatible_scenarios": ["æ„Ÿåˆ°ç–¼ç—›"]
                    }
                },
                "patient_wellcleansed.m4a": {
                    "filename": "patient_wellcleansed.m4a",
                    "text": "å¥½å•Šï¼Œä½ æ´—å¾—çœŸå¥½ï¼",
                    "duration": 2.0,
                    "conversation_phase": "ç¨‹åºå®Œæˆ",
                    "emotions": {
                        "primary": "æ„Ÿæ¿€",
                        "intensity": 2,
                        "secondary": ["æ„Ÿæ¿€", "å®‰å¿ƒ"]
                    },
                    "context_tags": {
                        "suitable_scenarios": ["ç¨‹åºå®Œæˆ", "è¡¨è¾¾æ„Ÿè°¢"],
                        "incompatible_scenarios": ["æ„Ÿåˆ°ç–¼ç—›"]
                    }
                    }
                };

                audioFilesList = Object.keys(audioMetadata);
                audioFilesLoaded = true;

                debugLog(`Loaded ${audioFilesList.length} audio files`);
                updateAudioFilesStatus(`âœ… éŸ³é »ç³»çµ±å°±ç·’ï¼å·²è¼‰å…¥ ${audioFilesList.length} å€‹éŸ³é »æ–‡ä»¶`);
                
                // Update audio files list in UI
                updateAudioFilesList();
                
                return true;
            } catch (error) {
                debugLog(`Failed to load audio metadata: ${error.message}`, 'error');
                updateAudioFilesStatus('âŒ éŸ³é »å…ƒæ•¸æ“šè¼‰å…¥å¤±æ•—');
                throw error;
            }
        }

        // Update audio files list display
        function updateAudioFilesList() {
            const listElement = document.getElementById('audioFilesList');
            if (audioFilesLoaded) {
                listElement.innerHTML = audioFilesList.map(filename => {
                    const metadata = audioMetadata[filename];
                    return `<div style="margin: 2px 0; padding: 2px; background: #f0f0f0; border-radius: 3px;">
                        <strong>${metadata.text}</strong><br>
                        <small>${metadata.conversation_phase} | ${metadata.emotions.primary}</small>
                    </div>`;
                }).join('');
            } else {
                listElement.innerHTML = 'Loading...';
            }
        }

        // Initialize AI Model (using rule-based system for better Chinese support)
        async function initializeAI() {
            try {
                debugLog('Initializing AI conversation system...');
                updateAIStatus('ğŸ¤– æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½å°è©±ç³»çµ±... Initializing intelligent conversation system...', 'loading');

                // Use rule-based AI for better Chinese/Cantonese support
                textGenerator = {
                    initialized: true,
                    generateResponse: function (context, options) {
                        return Promise.resolve([{
                            generated_text: generateIntelligentResponse(context)
                        }]);
                    }
                };

                debugLog('AI conversation system loaded successfully');
                updateAIStatus('âœ… æ™ºèƒ½å°è©±ç³»çµ±æº–å‚™å°±ç·’ï¼Intelligent conversation system ready!', 'ready');
                isAIReady = true;

                return true;
            } catch (error) {
                debugLog(`AI initialization failed: ${error.message}`, 'error');
                updateAIStatus('âŒ ç³»çµ±åˆå§‹åŒ–å¤±æ•— System initialization failed', 'error');
                isAIReady = false;
                throw error;
            }
        }

        // Intelligent audio file matching system
        function matchAudioFile(studentInput, conversationHistory) {
            debugLog(`Matching audio for input: "${studentInput}"`);
            debugLog(`Conversation length: ${conversationHistory.length}`);

            const input = studentInput.toLowerCase();
            const messageCount = conversationHistory.length;

            // Determine conversation phase based on message count and content
            let targetPhase = determineConversationPhase(messageCount, input, conversationHistory);
            debugLog(`Target conversation phase: ${targetPhase}`);

            // Get suitable audio files for current context
            const candidates = getSuitableAudioFiles(targetPhase, input);
            debugLog(`Found ${candidates.length} candidate audio files`);

            if (candidates.length === 0) {
                debugLog('No suitable audio files found, using fallback');
                return getFallbackAudioFile(targetPhase);
            }

            // Select best match based on context
            const selectedFile = selectBestAudioMatch(candidates, input, conversationHistory);
            debugLog(`Selected audio file: ${selectedFile}`);

            return selectedFile;
        }

        // Determine conversation phase
        function determineConversationPhase(messageCount, input, history) {
            const historyText = history.map(h => h.content).join(' ').toLowerCase();

            // Force progression based on message count
            if (messageCount <= 2) {
                if (input.includes('æŠ¤å£«') || input.includes('ä½•') || input.includes('å…¥') || input.includes('hello') || input.includes('hi')) {
                    return 'åˆæ¬¡æ¥è§¦';
                }
                return 'åˆæ¬¡æ¥è§¦';
            } else if (messageCount <= 5) {
                if (input.includes('é™ˆ') || input.includes('å') || input.includes('èº«ä»½') || input.includes('ç¡®è®¤') || input.includes('check')) {
                    return 'èº«ä»½ç¡®è®¤';
                }
                if (historyText.includes('é™ˆ') || historyText.includes('èº«ä»½')) {
                    return 'ç¨‹åºè§£é‡Š';
                }
                return 'èº«ä»½ç¡®è®¤';
            } else if (messageCount <= 8) {
                if (input.includes('æ´—') || input.includes('ä¼¤å£') || input.includes('æ¢è¯') || input.includes('å¼€å§‹') || input.includes('procedure')) {
                    return 'ç¨‹åºè§£é‡Š';
                }
                return 'ç¨‹åºè§£é‡Š';
            } else {
                if (input.includes('å®Œæˆ') || input.includes('ç»“æŸ') || input.includes('è°¢è°¢') || input.includes('thank')) {
                    return 'ç¨‹åºå®Œæˆ';
                }
                return 'ç¨‹åºè¿›è¡Œ';
            }
        }

        // Get suitable audio files for current phase and input
        function getSuitableAudioFiles(targetPhase, input) {
            const candidates = [];

            for (const [filename, metadata] of Object.entries(audioMetadata)) {
                // Check if phase matches
                if (metadata.conversation_phase === targetPhase) {
                    candidates.push({ filename, metadata, score: 10 });
                    continue;
                }

                // Check suitable scenarios
                const scenarios = metadata.context_tags?.suitable_scenarios || [];
                let scenarioMatch = false;

                for (const scenario of scenarios) {
                    if (matchesScenario(input, scenario)) {
                        candidates.push({ filename, metadata, score: 8 });
                        scenarioMatch = true;
                        break;
                    }
                }

                if (scenarioMatch) continue;

                // Check incompatible scenarios (negative scoring)
                const incompatibleScenarios = metadata.context_tags?.incompatible_scenarios || [];
                let isIncompatible = false;

                for (const incompatible of incompatibleScenarios) {
                    if (matchesScenario(input, incompatible)) {
                        isIncompatible = true;
                        break;
                    }
                }

                if (!isIncompatible) {
                    candidates.push({ filename, metadata, score: 3 });
                }
            }

            return candidates.sort((a, b) => b.score - a.score);
        }

        // Check if input matches a scenario
        function matchesScenario(input, scenario) {
            const scenarioMap = {
                'é™Œç”Ÿäººè¿›å…¥': ['æŠ¤å£«', 'ä½•', 'å…¥', 'hello', 'hi', 'è¿›æ¥', 'ä½ ', 'æˆ‘', 'å¥½', 'åŒå­¦', 'å­¦ç”Ÿ'],
                'æ„Ÿåˆ°ç–¼ç—›': ['ç—›', 'pain', 'hurt', 'ä¸èˆ’æœ', 'éš¾å—', 'ç–¼'],
                'å¼€å§‹æ“ä½œ': ['æ´—', 'å¼€å§‹', 'æ¢è¯', 'start', 'procedure', 'ç°åœ¨', 'å¸®', 'æ²»ç–—', 'å¤„ç†', 'çœ‹', 'æ£€æŸ¥'],
                'ç¨‹åºè§£é‡Šä¸­': ['è§£é‡Š', 'ä»€ä¹ˆ', 'what', 'how', 'ä¸ºä»€ä¹ˆ', 'æ€ä¹ˆ', 'å¯ä»¥', 'éœ€è¦', 'è¦'],
                'éœ€è¦èº«ä»½ç¡®è®¤': ['é™ˆ', 'åå­—', 'èº«ä»½', 'name', 'check', 'å«', 'ç¡®è®¤', 'æ˜¯', 'ä½ '],
                'æ“ä½œè¿›è¡Œä¸­': ['è¿›è¡Œ', 'ç»§ç»­', 'ç°åœ¨', 'continue', 'å¥½', 'å¼€å§‹', 'è¿™æ ·', 'æ…¢æ…¢'],
                'ç¨‹åºå®Œæˆ': ['å®Œæˆ', 'ç»“æŸ', 'finish', 'done', 'å¥½äº†', 'å¯ä»¥', 'æ€ä¹ˆæ ·', 'æ„Ÿè§‰'],
                'è¡¨è¾¾æ„Ÿè°¢': ['è°¢è°¢', 'æ„Ÿè°¢', 'thank', 'å¥½', 'å¾ˆå¥½', 'ä¸é”™', 'è¾›è‹¦'],
                'å…³å¿ƒåç»­': ['æ€ä¹ˆæ ·', 'èˆ’æœ', 'how', 'feel', 'è¿˜', 'ç°åœ¨', 'æœ‰æ²¡æœ‰']
            };

            const keywords = scenarioMap[scenario] || [];
            return keywords.some(keyword => input.includes(keyword));
        }

        // Select best audio match
        function selectBestAudioMatch(candidates, input, history) {
            if (candidates.length === 0) return null;

            // Filter by emotion if input suggests pain
            if (input.includes('ç—›') || input.includes('pain')) {
                const painCandidates = candidates.filter(c => 
                    c.metadata.emotions.primary === 'ç–¼ç—›' || 
                    c.metadata.emotions.secondary?.includes('ç–¼ç—›')
                );
                if (painCandidates.length > 0) {
                    return painCandidates[Math.floor(Math.random() * painCandidates.length)].filename;
                }
            }

            // Filter by cooperation level
            if (input.includes('å¥½') || input.includes('ok') || input.includes('å¯ä»¥')) {
                const cooperativeCandidates = candidates.filter(c => 
                    c.metadata.emotions.primary === 'é…åˆ' || 
                    c.metadata.emotions.secondary?.includes('é…åˆ')
                );
                if (cooperativeCandidates.length > 0) {
                    return cooperativeCandidates[Math.floor(Math.random() * cooperativeCandidates.length)].filename;
                }
            }

            // Default: select from top candidates with some randomness
            const topCandidates = candidates.slice(0, Math.min(3, candidates.length));
            return topCandidates[Math.floor(Math.random() * topCandidates.length)].filename;
        }

        // Get fallback audio file
        function getFallbackAudioFile(targetPhase) {
            const fallbackMap = {
                'åˆæ¬¡æ¥è§¦': 'patient_greet_painful.m4a',
                'èº«ä»½ç¡®è®¤': "patient_I'mChenXiaoLi.m4a",
                'ç¨‹åºè§£é‡Š': 'patient_helpmecleanse.m4a',
                'ç¨‹åºè¿›è¡Œ': 'patient_painful_abit.m4a',
                'ç¨‹åºå®Œæˆ': 'patient_nothingspecial_thanks.m4a'
            };

            return fallbackMap[targetPhase] || 'patient_first_meeting_procedure_okay.m4a';
        }

        // Play audio file
        async function playAudioFile(filename) {
            try {
                debugLog(`Playing audio file: ${filename}`);
                
                // In a real implementation, you would load the actual audio file
                // For now, we'll simulate by creating a text-to-speech placeholder
                const metadata = audioMetadata[filename];
                if (!metadata) {
                    throw new Error(`Audio metadata not found for: ${filename}`);
                }

                // Create audio element (in real implementation, this would be the actual .m4a file)
                const audio = new Audio();
                
                // For demo purposes, we'll use a data URL or you could host the files
                audio.src = `./audio/${filename}`;
                
                // Since we don't have the actual files, we'll create a mock audio
                // In production, you would uncomment the line above and comment out the mock below
                
                // MOCK IMPLEMENTATION - Remove this in production
                //const mockAudioData = createMockAudio(metadata.text, metadata.duration);
                //audio.src = mockAudioData;
                
                audio.volume = currentVolume;
                audio.preload = 'auto';

                // Audio event handlers
                audio.oncanplay = () => {
                    debugLog(`Audio ${filename} ready to play`);
                };

                audio.onplay = () => {
                    debugLog(`Audio ${filename} started playing`);
                };

                audio.onended = () => {
                    debugLog(`Audio ${filename} finished playing`);
                };

                audio.onerror = (e) => {
                    debugLog(`Audio ${filename} playback error: ${e.message || 'Unknown error'}`, 'error');
                };

                // Play audio with user interaction handling
                await playAudioWithInteractionHandling(audio, filename);

                return metadata.text; // Return the text for display

            } catch (error) {
                debugLog(`Failed to play audio file ${filename}: ${error.message}`, 'error');
                throw error;
            }
        }

        // Handle audio playback with browser interaction requirements
        async function playAudioWithInteractionHandling(audio, filename) {
            try {
                // Ensure audio context is active
                if (audioContext && audioContext.state === 'suspended') {
                    await audioContext.resume();
                    debugLog('Audio context resumed for playback');
                }

                // Try to play directly
                await audio.play();
                debugLog(`Audio ${filename} played successfully`);

            } catch (error) {
                debugLog(`Direct audio play failed: ${error.message}`, 'error');
                
                // Queue for next user interaction
                const playOnInteraction = () => {
                    audio.play().then(() => {
                        debugLog(`Audio ${filename} played after user interaction`);
                        document.removeEventListener('click', playOnInteraction);
                        document.removeEventListener('keydown', playOnInteraction);
                        document.removeEventListener('touchstart', playOnInteraction);
                    }).catch(e => {
                        debugLog(`Play after interaction failed: ${e.message}`, 'error');
                    });
                };

                document.addEventListener('click', playOnInteraction, { once: true });
                document.addEventListener('keydown', playOnInteraction, { once: true });
                document.addEventListener('touchstart', playOnInteraction, { once: true });
            }
        }

        // Create mock audio data (remove in production)
        function createMockAudio(text, duration) {
            // Create a simple data URL for testing
            // In production, replace with actual file loading
            return 'data:audio/wav;base64,UklGRigAAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQEAAAA=';
        }

        // Generate intelligent response using rule-based AI
        function generateIntelligentResponse(context) {
            const lines = context.split('\n');
            const lastStudentInput = lines[lines.length - 2]?.replace('è­·å£«: ', '') || '';
            const conversationHistory = lines.slice(2, -2);

            debugLog(`Generating response for: ${lastStudentInput}`);
            debugLog(`Conversation length: ${conversationHistory.length}`);

            // Use audio file matching system
            const selectedAudioFile = matchAudioFile(lastStudentInput, conversationHistory.map(line => ({
                role: line.includes('è­·å£«:') ? 'student' : 'patient',
                content: line.replace(/^(è­·å£«|ç—…äºº): /, '')
            })));

            // Get text from selected audio file
            const audioMetadata = audioMetadata[selectedAudioFile];
            if (audioMetadata) {
                return audioMetadata.text;
            }

            // Fallback to original rule-based system
            return generateRuleBasedResponse(lastStudentInput, conversationHistory);
        }

        // Fallback rule-based response
        function generateRuleBasedResponse(studentInput, history) {
            const input = studentInput.toLowerCase();
            const messageCount = history.length;

            // Simple rule-based responses as fallback
            if (messageCount <= 2) {
                return "ä½ è¿›æ¥å§ã€‚æˆ‘å¥½ç—›å•Šï¼";
            } else if (messageCount <= 5) {
                if (input.includes('æŠ¤å£«') || input.includes('ä½•')) {
                    return "æˆ‘å«é™ˆå°ä¸½ã€‚";
                }
                return "å¥½å•Š";
            } else if (messageCount <= 8) {
                return "å¥½å•Šï¼Œä½ å¸®æˆ‘æ´—å•Šã€‚";
            } else {
                if (input.includes('ç—›') || input.includes('pain')) {
                    return "æœ‰å°‘å°‘ç—›å•Šã€‚";
                }
                return "å“¦ï¼Œ okay å–ï¼Œå†‡ä¹ˆç‰¹åˆ«å¾èˆ’æœï¼Œå¾è¯¥æ™’ï¼";
            }
        }

        // Update AI status display
        function updateAIStatus(message, type) {
            const aiStatus = document.getElementById('aiStatus');
            aiStatus.innerHTML = type === 'loading' ?
                `<span class="loading-spinner"></span> ${message}` :
                message;
            aiStatus.className = `ai-status ${type}`;
        }

        // Update audio files status display
        function updateAudioFilesStatus(message) {
            const audioFilesStatus = document.getElementById('audioFilesStatus');
            audioFilesStatus.textContent = message;
        }

        // Audio context initialization
        async function enableAudioContext() {
            try {
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    debugLog('Audio context created');
                }

                debugLog(`Audio context state: ${audioContext.state}`);

                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                    debugLog('Audio context resumed successfully');
                    updateAudioStatus('ğŸ”Š éŸ³é »å·²å•Ÿç”¨ Audio enabled', 'success');
                } else {
                    debugLog('Audio context already running');
                    updateAudioStatus('âœ… éŸ³é »å·²å°±ç·’ Audio ready', 'success');
                }

                // Create silent test
                const oscillator = audioContext.createOscillator();
                const gainNode = audioContext.createGain();
                
                oscillator.connect(gainNode);
                gainNode.connect(audioContext.destination);
                
                gainNode.gain.setValueAtTime(0, audioContext.currentTime);
                oscillator.frequency.setValueAtTime(440, audioContext.currentTime);
                
                oscillator.start();
                oscillator.stop(audioContext.currentTime + 0.001);
                
                debugLog('Silent audio test completed');

            } catch (error) {
                debugLog(`Audio context enable failed: ${error.message}`, 'error');
                updateAudioStatus(`âŒ éŸ³é »å•Ÿç”¨å¤±æ•—: ${error.message}`, 'error');
            }
        }

        // Test Audio with file system
        async function testAudio() {
            try {
                debugLog('Testing audio file system...');
                updateAudioStatus('ğŸ”„ æ­£åœ¨æ¸¬è©¦éŸ³é »æ–‡ä»¶ç³»çµ±... Testing audio file system...', 'warning');

                // Test with a simple greeting audio
                const testFileName = 'å¥½å•Š.m4a';
                const responseText = await playAudioFile(testFileName);
                
                updateAudioStatus('âœ… éŸ³é »æ–‡ä»¶ç³»çµ±æ¸¬è©¦æˆåŠŸï¼Audio file system test successful!', 'success');

            } catch (error) {
                debugLog(`Audio test failed: ${error.message}`, 'error');
                updateAudioStatus(`âŒ éŸ³é »æ¸¬è©¦å¤±æ•—: ${error.message}`, 'error');
            }
        }

        // Update audio status display
        function updateAudioStatus(message, type) {
            const audioStatus = document.getElementById('audioStatus');
            audioStatus.textContent = message;
            audioStatus.className = `audio-status ${type}`;
        }

        // Process student input with audio file system
        async function processStudentInput(transcript) {
        debugLog(`Processing student input: ${transcript}`);

        try {
            // Get matched audio file
            const selectedAudioFile = matchAudioFile(transcript, conversationHistory);
            const audioMetadata_file = audioMetadata[selectedAudioFile];

            if (!audioMetadata_file) {
                throw new Error(`Audio metadata not found for: ${selectedAudioFile}`);
            }

            // Reset failure count on successful match
            conversationFailureCount = 0;
            lastFailedPhase = null;

            // Simulate thinking time
            setTimeout(async () => {
                debugLog(`Patient responding with audio: ${selectedAudioFile}`);

                // Generate contextual notes
                const notes = generateContextualNotes(transcript, conversationHistory);

                const responseText = audioMetadata_file.text;
                addMessage('patient', responseText, `ğŸµ æ’­æ”¾: ${selectedAudioFile} | ${notes}`);
                conversationHistory.push({ role: 'patient', content: responseText });

                // Play the actual audio file
                await playAudioFile(selectedAudioFile);

            }, 1500 + Math.random() * 1000);

        } catch (error) {
            debugLog(`Audio file system failed: ${error.message}`, 'error');
            
            conversationFailureCount++;
            const currentPhase = determineConversationPhase(conversationHistory.length, transcript.toLowerCase(), conversationHistory);
            
            if (conversationFailureCount === 1) {
                // ç¬¬ä¸€æ¬¡å¤±æ•—ï¼šçµ¦æ–‡å­—æç¤º
                const textHint = getTextHintForPhase(currentPhase, transcript);
                addMessage('patient', textHint, "ğŸ’¬ æ–‡å­—æç¤º - ç³»çµ±å»ºè­°å›æ‡‰æ–¹å‘");
                conversationHistory.push({ role: 'patient', content: textHint });
                lastFailedPhase = currentPhase;
                
            } else if (conversationFailureCount >= 2 && lastFailedPhase === currentPhase) {
                // ç¬¬äºŒæ¬¡å¤±æ•—ä¸”åœ¨åŒä¸€éšæ®µï¼šå¼·åˆ¶é€²å…¥ä¸‹ä¸€éšæ®µ
                debugLog('Forcing progression to next conversation phase');
                const nextPhaseResponse = forceNextPhaseProgression(currentPhase);
                addMessage('patient', nextPhaseResponse, "â­ï¸ è‡ªå‹•é€²å…¥ä¸‹ä¸€éšæ®µ");
                conversationHistory.push({ role: 'patient', content: nextPhaseResponse });
                
                // Reset counters
                conversationFailureCount = 0;
                lastFailedPhase = null;
            }
        }
    }
        // ç²å–éšæ®µæ–‡å­—æç¤º
        function getTextHintForPhase(phase, studentInput) {
            const hints = {
                'åˆæ¬¡æ¥è§¦': 'ä½ å¥½ï¼Œæˆ‘æ˜¯è­·ç†å­¸ç”Ÿã€‚è«‹å•ä½ æ˜¯é™³å°ä¸½å¥³å£«å—ï¼Ÿ',
                'èº«ä»½ç¡®è®¤': 'å¥½çš„ï¼Œæˆ‘éœ€è¦ç¢ºèªä¸€ä¸‹ä½ çš„èº«ä»½ä¿¡æ¯ã€‚è«‹å•ä½ çš„å…¨åæ˜¯ä»€éº¼ï¼Ÿ',
                'ç¨‹åºè§£é‡Š': 'ç¾åœ¨æˆ‘è¦å¹«ä½ æ¸…æ´—å‚·å£å’Œæ›è—¥ã€‚é€™å€‹éç¨‹å¯èƒ½æœƒæœ‰ä¸€é»ä¸èˆ’æœï¼Œä½†æˆ‘æœƒç›¡é‡è¼•æŸ”ã€‚',
                'ç¨‹åºè¿›è¡Œ': 'æˆ‘ç¾åœ¨é–‹å§‹æ¸…æ´—å‚·å£äº†ã€‚å¦‚æœæ„Ÿåˆ°ç–¼ç—›è«‹å‘Šè¨´æˆ‘ã€‚',
                'ç¨‹åºå®Œæˆ': 'å¥½çš„ï¼Œå‚·å£è™•ç†å®Œæˆäº†ã€‚ä½ ç¾åœ¨æ„Ÿè¦ºæ€éº¼æ¨£ï¼Ÿ'
            };
            
            return hints[phase] || 'è«‹ç¹¼çºŒèˆ‡ç—…äººæºé€šï¼Œæ³¨æ„å¥¹çš„æ„Ÿå—å’Œéœ€è¦ã€‚';
        }

        // å¼·åˆ¶é€²å…¥ä¸‹ä¸€éšæ®µ
        function forceNextPhaseProgression(currentPhase) {
            const progressionMap = {
                'åˆæ¬¡æ¥è§¦': 'patient_first_meeting_procedure_okay.m4a',
                'èº«ä»½ç¡®è®¤': "patient_I'mChenXiaoLi.m4a", 
                'ç¨‹åºè§£é‡Š': 'patient_helpmecleanse.m4a',
                'ç¨‹åºè¿›è¡Œ': 'patient_painful_abit.m4a',
                'ç¨‹åºå®Œæˆ': 'patient_nothingspecial_thanks.m4a'
            };
            
            const audioFile = progressionMap[currentPhase];
            if (audioFile && audioMetadata[audioFile]) {
                return audioMetadata[audioFile].text;
            }
            
            // é»˜èªå›æ‡‰
            return 'å¥½å•Šï¼Œæˆ‘å€‘ç¹¼çºŒå§ã€‚';
        }
        
        // Generate contextual notes
        function generateContextualNotes(studentInput, history) {
            const input = studentInput.toLowerCase();
            const conversationLength = history.length;

            if (conversationLength <= 2) {
                if (input.includes('æŠ¤å£«') || input.includes('ä½•') || input.includes('å…¥')) {
                    return "åˆæ¬¡æ¥è§¦é˜¶æ®µ - å»ºç«‹ä¸“ä¸šå…³ç³»";
                }
            } else if (conversationLength <= 6) {
                if (input.includes('é™ˆ') || input.includes('èº«ä»½') || input.includes('å')) {
                    return "ç—…äººèº«ä»½ç¡®è®¤éªŒè¯ - é‡è¦å®‰å…¨ç¨‹åº";
                }
            } else if (conversationLength <= 10) {
                if (input.includes('æ´—') || input.includes('æ¢è¯') || input.includes('ä¼¤å£')) {
                    return "ç¨‹åºè§£é‡Šé˜¶æ®µ - å»ºç«‹ç—…äººä¿¡å¿ƒ";
                }
            }

            if (input.includes('ç—›') || input.includes('pain')) {
                return "ç—…äººè¡¨è¾¾ç–¼ç—›æ‹…å¿§ - éœ€è¦å…±æƒ…å’Œå®‰æŠš";
            } else if (input.includes('ç´§å¼ ') || input.includes('scared')) {
                return "ç—…äººæ˜¾ç¤ºç„¦è™‘ - éœ€è¦é•‡é™æ–¹æ³•";
            }

            return "å¯¹è¯è¿›è¡Œä¸­ - ä¿æŒä¸“ä¸šèæ´½å…³ç³»";
        }

        // Rest of the code remains the same as the original...
        // Initialize speech recognition, start/stop session, etc.

        // Initialize speech recognition
        async function initSpeechRecognition() {
            if ('webkitSpeechRecognition' in window) {
                recognition = new webkitSpeechRecognition();
                debugLog('Using webkitSpeechRecognition');
            } else if ('SpeechRecognition' in window) {
                recognition = new SpeechRecognition();
                debugLog('Using SpeechRecognition');
            } else {
                throw new Error('Speech recognition not supported in this browser. Please use Chrome or Edge.');
            }

            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = selectedRecognitionLang;

            recognition.onresult = function (event) {
                const result = event.results[event.results.length - 1];
                if (result.isFinal) {
                    const transcript = result[0].transcript.trim();
                    debugLog(`Speech recognized: ${transcript}`);
                    addMessage('student', transcript);
                    conversationHistory.push({ role: 'student', content: transcript });
                    processStudentInput(transcript);
                }
            };

            recognition.onerror = function (event) {
                debugLog(`Speech recognition error: ${event.error}`, 'error');
                if (event.error === 'no-speech' && isRecording) {
                    setTimeout(() => {
                        try {
                            recognition.start();
                            debugLog('Speech recognition restarted after no-speech');
                        } catch (e) {
                            debugLog(`Recognition restart failed: ${e.message}`, 'error');
                        }
                    }, 1000);
                } else if (event.error === 'not-allowed') {
                    alert('éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•ã€‚è«‹åœ¨ç€è¦½å™¨è¨­ç½®ä¸­å…è¨±éº¥å…‹é¢¨è¨ªå•ã€‚Microphone access denied. Please allow microphone access in browser settings.');
                }
            };

            recognition.onend = function () {
                debugLog('Speech recognition ended');
                if (isRecording) {
                    try {
                        recognition.start();
                        debugLog('Speech recognition restarted');
                    } catch (e) {
                        debugLog(`Recognition restart failed: ${e.message}`, 'error');
                    }
                }
            };

            debugLog('Speech recognition configured successfully');
        }
        // Create conversation context for AI
        function createPatientContext(studentInput, history) {
            const context = `ä½ æ˜¯65æ­²çš„ç—…äººé™ˆå°ä¸½ï¼Œè…¿éƒ¨æœ‰å‚·å£éœ€è¦è­·ç†ã€‚ä½ æ€§æ ¼å¥è«‡ä½†æœ‰é»ç·Šå¼µï¼Œæœƒé…åˆæ²»ç™‚ä½†æœ‰ç–¼ç—›åæ‡‰ï¼Œæœƒæ··åˆå»£æ±è©±èªªè©±ã€‚

            ä¹‹å‰çš„å°è©±ï¼š
            ${history.map(h => `${h.role === 'student' ? 'è­·å£«' : 'ç—…äºº'}: ${h.content}`).join('\n')}
            ç¾åœ¨è­·å£«èªªï¼š${studentInput}
            è«‹ä»¥ç—…äººçš„èº«ä»½å›æ‡‰ï¼Œä¿æŒè§’è‰²ä¸€è‡´æ€§ï¼Œè¡¨é”å¯èƒ½çš„ç–¼ç—›ã€æ“”æ†‚æˆ–æ„Ÿè¬ã€‚`;

            return context;
        }

        // Start training session
        async function startSession() {
            try {
                debugLog('Starting training session with audio files system');

                // Check if systems are ready
                if (!isAIReady) {
                    alert('AIæ¨¡å‹å°šæœªå°±ç·’ï¼Œè«‹ç¨ç­‰ç‰‡åˆ»å†è©¦ï¼\nAI model not ready yet, please wait and try again!');
                    return;
                }

                if (!audioFilesLoaded) {
                    alert('éŸ³é »æ–‡ä»¶ç³»çµ±å°šæœªå°±ç·’ï¼Œè«‹ç¨ç­‰ç‰‡åˆ»å†è©¦ï¼\nAudio files system not ready yet, please wait and try again!');
                    return;
                }

                if (!audioContext || audioContext.state === 'suspended') {
                    alert('è«‹å…ˆé»æ“Š"å•Ÿç”¨éŸ³é »"æŒ‰éˆ•ï¼Œç¢ºä¿éŸ³é »æ­£å¸¸å·¥ä½œï¼\nPlease click "Enable Audio" button first to ensure audio works properly!');
                    return;
                }

                isRecording = true;
                audioChunks = [];
                conversationHistory = [];

                // Clear previous conversation
                document.getElementById('messages').innerHTML = '';
                document.getElementById('conversation').style.display = 'block';

                // Start audio recording
                debugLog('Requesting microphone access');
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                debugLog('Microphone access granted');

                mediaRecorder = new MediaRecorder(stream, {
                    mimeType: 'audio/webm;codecs=opus'
                });

                mediaRecorder.ondataavailable = function (event) {
                    audioChunks.push(event.data);
                };

                mediaRecorder.onstop = function () {
                    debugLog('Media recorder stopped, creating download link');
                    const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                    createDownloadLink(audioBlob);
                };

                mediaRecorder.start();
                debugLog('Audio recording started');

                // Start speech recognition
                debugLog('Starting speech recognition');
                recognition.start();

                // Start timer
                sessionStartTime = Date.now();
                startTimer();

                // Initial scenario setup with audio files
                setTimeout(async () => {
                    debugLog('Playing initial patient greeting from audio files');

                    // Use the greeting audio file
                    const initialAudioFile = 'patient_greet_painful.m4a';
                    const responseText = await playAudioFile(initialAudioFile);
                    
                    addMessage('patient', responseText, `ğŸµ æ’­æ”¾åˆå§‹éŸ³é »: ${initialAudioFile} - è®­ç»ƒå¼€å§‹`);
                    conversationHistory.push({ role: 'patient', content: responseText });

                }, 3000);

                updateStatus('ğŸ™ï¸ éŸ³é »æ–‡ä»¶æ™ºèƒ½è¨“ç·´é€²è¡Œä¸­... Audio files intelligent training in progress', 'recording');
                document.getElementById('startBtn').disabled = true;
                document.getElementById('stopBtn').disabled = false;
                debugLog('Training session started successfully with audio files system');

            } catch (error) {
                debugLog(`Error starting session: ${error.message}`, 'error');
                console.error('Error starting session:', error);

                if (error.name === 'NotAllowedError') {
                    alert('éº¥å…‹é¢¨æ¬Šé™è¢«æ‹’çµ•ã€‚è«‹åœ¨ç€è¦½å™¨è¨­ç½®ä¸­å…è¨±éº¥å…‹é¢¨è¨ªå•ã€‚Microphone access denied. Please allow microphone access in browser settings.');
                } else {
                    alert('ç„¡æ³•ä½¿ç”¨éº¥å…‹é¢¨ï¼Œè«‹æª¢æŸ¥æ¬Šé™è¨­å®š Error accessing microphone. Please check permissions.');
                }
            }
        }

        // Stop training session
        function stopSession() {
            debugLog('Stopping training session');
            isRecording = false;

            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                debugLog('Media recorder stopped');
            }

            if (recognition) {
                recognition.stop();
                debugLog('Speech recognition stopped');
            }

            stopTimer();

            updateStatus('âœ… éŸ³é »æ–‡ä»¶æ™ºèƒ½è¨“ç·´å®Œæˆï¼Audio files intelligent training completed!', 'ready');
            document.getElementById('startBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;

            document.getElementById('downloadSection').style.display = 'block';
            debugLog('Training session stopped successfully');
        }

        // Add message to conversation
        function addMessage(speaker, text, notes = null) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${speaker}-message`;

            const header = document.createElement('div');
            header.className = 'message-header';
            header.innerHTML = speaker === 'student' ?
                'ğŸ‘©â€âš•ï¸ è­·ç†å­¸ç”Ÿä½•åŒå­¸ Student Ho' :
                'ğŸ‘¨ ç—…äººé™ˆå¥³å£« Patient Ms. Chan';

            const content = document.createElement('div');
            content.className = 'message-content';
            content.innerHTML = `<span class="cantonese-text">${text}</span>`;

            messageDiv.appendChild(header);
            messageDiv.appendChild(content);

            if (notes && speaker === 'patient') {
                const notesDiv = document.createElement('div');
                notesDiv.className = 'procedure-notes';
                notesDiv.textContent = `ğŸ“ ${notes}`;
                messageDiv.appendChild(notesDiv);
            }

            document.getElementById('messages').appendChild(messageDiv);
            document.getElementById('messages').scrollTop = document.getElementById('messages').scrollHeight;
        }

        // Timer functions
        function startTimer() {
            timerInterval = setInterval(() => {
                const elapsed = Date.now() - sessionStartTime;
                const minutes = Math.floor(elapsed / 60000);
                const seconds = Math.floor((elapsed % 60000) / 1000);
                document.getElementById('timer').textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
            }, 1000);
        }

        function stopTimer() {
            if (timerInterval) {
                clearInterval(timerInterval);
            }
        }

        // Create download link for recording
        function createDownloadLink(audioBlob) {
            const url = URL.createObjectURL(audioBlob);
            document.getElementById('downloadBtn').onclick = function () {
                const a = document.createElement('a');
                a.href = url;
                a.download = `audio-files-nursing-training-${new Date().toISOString().slice(0, 19)}.webm`;
                a.click();
            };
        }

        // Update status display
        function updateStatus(message, type) {
            const statusEl = document.getElementById('status');
            statusEl.textContent = message;
            statusEl.className = `status ${type}`;
        }

        // Debug logging function
        function debugLog(message, type = 'info') {
            console.log(`[${type.toUpperCase()}] ${message}`);

            if (debugMode) {
                const debugLogs = document.getElementById('debugLogs');
                const logDiv = document.createElement('div');
                logDiv.className = `debug-log debug-${type}`;
                logDiv.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;
                debugLogs.appendChild(logDiv);
                debugLogs.scrollTop = debugLogs.scrollHeight;
            }
        }

        function clearDebugLogs() {
            document.getElementById('debugLogs').innerHTML = '';
        }

        function toggleDebug() {
            debugMode = !debugMode;
            const debugSection = document.getElementById('debugSection');
            debugSection.style.display = debugMode ? 'block' : 'none';
        }

        // Volume control
        const volumeSlider = document.getElementById('volumeSlider');
        const volumeDisplay = document.getElementById('volumeDisplay');

        volumeSlider.addEventListener('input', (e) => {
            currentVolume = parseFloat(e.target.value);
            volumeDisplay.textContent = Math.round(currentVolume * 100) + '%';
            debugLog(`Volume set to: ${Math.round(currentVolume * 100)}%`);
        });

        // Settings
        document.getElementById('responseMode').addEventListener('change', (e) => {
            selectedResponseMode = e.target.value;
            debugLog(`Response mode changed to: ${selectedResponseMode}`);
        });

        document.getElementById('recognitionLang').addEventListener('change', (e) => {
            selectedRecognitionLang = e.target.value;
            debugLog(`Recognition language changed to: ${selectedRecognitionLang}`);
            if (recognition) {
                recognition.lang = selectedRecognitionLang;
            }
        });

        // Initialize the application
        async function init() {
            try {
                debugLog('Starting initialization with audio files system...');
                updateStatus('æ­£åœ¨åˆå§‹åŒ–éŸ³é »æ–‡ä»¶ç³»çµ±å’ŒAI... Initializing audio files system and AI...', 'loading');

                // Enable audio context on first user interaction
                document.addEventListener('click', enableAudioContext, { once: true });

                // Load audio metadata first
                debugLog('Loading audio metadata...');
                await loadAudioMetadata();
                debugLog('Audio metadata loaded successfully');

                // Initialize AI model
                debugLog('Initializing AI model...');
                await initializeAI();
                debugLog('AI model initialized successfully');

                // Initialize speech recognition
                updateStatus('æ­£åœ¨è¼‰å…¥èªéŸ³è­˜åˆ¥... Loading speech recognition...', 'loading');
                debugLog('Initializing speech recognition...');
                await initSpeechRecognition();
                debugLog('Speech recognition initialized successfully');

                updateStatus('ğŸ‰ éŸ³é »æ–‡ä»¶ç³»çµ±æº–å‚™å°±ç·’ï¼é»æ“Š"æ¸¬è©¦éŸ³é »"ç¢ºèªè²éŸ³æ­£å¸¸ï¼Audio files system ready!', 'ready');
                document.getElementById('startBtn').disabled = false;
                debugLog('Initialization completed successfully with audio files system');

            } catch (error) {
                console.error('Initialization error:', error);
                debugLog(`Initialization failed: ${error.message}`, 'error');
                updateStatus(`âš ï¸ åˆå§‹åŒ–å¤±æ•— Initialization failed: ${error.message}`, 'error');
            }
        }

        // Make functions available globally
        window.testAudio = testAudio;
        window.enableAudioContext = enableAudioContext;
        window.clearDebugLogs = clearDebugLogs;
        window.toggleDebug = toggleDebug;

        // Event listeners
        document.getElementById('startBtn').addEventListener('click', startSession);
        document.getElementById('stopBtn').addEventListener('click', stopSession);

        // Enhanced audio initialization
        let audioContextReady = false;

        const enableAudioOnInteraction = async () => {
            if (!audioContextReady) {
                await enableAudioContext();
                audioContextReady = true;
                debugLog('Audio context fully activated by user interaction');
                
                // Play silent test audio
                try {
                    const silentAudio = new Audio('data:audio/wav;base64,UklGRigAAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQEAAAA=');
                    silentAudio.volume = 0;
                    await silentAudio.play();
                    debugLog('Silent audio test successful');
                } catch (e) {
                    debugLog('Silent audio test failed (this is usually OK)');
                }
            }
        };

        // Listen for interaction events
        ['click', 'keydown', 'touchstart', 'mousedown'].forEach(eventType => {
            document.addEventListener(eventType, enableAudioOnInteraction, { once: true });
        });

        // Initialize on page load
        init();
    </script>
</body>