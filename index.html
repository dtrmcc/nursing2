<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>è­·ç†å­¸ç”Ÿå‚·å£è­·ç†è¨“ç·´ - Nursing Wound Care Training</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            color: #333;
        }
        
        .container {
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 40px rgba(0,0,0,0.1);
        }
        
        .header {
            text-align: center;
            margin-bottom: 30px;
        }
        
        .header h1 {
            color: #2c3e50;
            margin-bottom: 10px;
            font-size: 28px;
        }
        
        .header h2 {
            color: #34495e;
            margin-bottom: 15px;
            font-size: 18px;
        }
        
        .azure-badge {
            background: #0078d4;
            color: white;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 12px;
            font-weight: bold;
            margin-left: 10px;
        }
        
        .scenario-info {
            background: #f8f9fa;
            padding: 20px;
            border-radius: 10px;
            margin-bottom: 25px;
            border-left: 4px solid #3498db;
        }
        
        .patient-info {
            background: #e8f5e8;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #27ae60;
        }
        
        .controls {
            display: flex;
            gap: 15px;
            justify-content: center;
            margin: 25px 0;
            flex-wrap: wrap;
        }
        
        .btn {
            padding: 12px 24px;
            border: none;
            border-radius: 8px;
            font-size: 16px;
            cursor: pointer;
            transition: all 0.3s ease;
            font-weight: 600;
        }
        
        .btn-primary {
            background: #3498db;
            color: white;
        }
        
        .btn-primary:hover {
            background: #2980b9;
            transform: translateY(-2px);
        }
        
        .btn-danger {
            background: #e74c3c;
            color: white;
        }
        
        .btn-danger:hover {
            background: #c0392b;
        }
        
        .btn:disabled {
            background: #bdc3c7;
            cursor: not-allowed;
            transform: none;
        }
        
        .status {
            text-align: center;
            padding: 15px;
            border-radius: 8px;
            margin: 20px 0;
            font-weight: 600;
        }
        
        .status.loading {
            background: #fff3cd;
            color: #856404;
        }
        
        .status.ready {
            background: #d4edda;
            color: #155724;
        }
        
        .status.recording {
            background: #f8d7da;
            color: #721c24;
            animation: pulse 1.5s infinite;
        }
        
        .status.error {
            background: #f8d7da;
            color: #721c24;
        }
        
        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.7; }
        }
        
        .timer {
            font-size: 24px;
            font-weight: bold;
            text-align: center;
            color: #2c3e50;
            margin: 20px 0;
        }
        
        .conversation {
            background: #f8f9fa;
            border-radius: 10px;
            padding: 20px;
            margin: 20px 0;
            max-height: 400px;
            overflow-y: auto;
        }
        
        .message {
            margin: 15px 0;
            padding: 15px;
            border-radius: 8px;
            line-height: 1.5;
        }
        
        .student-message {
            background: #e3f2fd;
            margin-left: 20px;
            border-left: 4px solid #2196f3;
        }
        
        .patient-message {
            background: #fff3e0;
            margin-right: 20px;
            border-left: 4px solid #ff9800;
        }
        
        .message-header {
            font-weight: bold;
            margin-bottom: 8px;
            color: #2c3e50;
        }
        
        .message-content {
            font-size: 16px;
        }
        
        .cantonese-text {
            color: #c0392b;
            font-weight: 500;
        }
        
        .procedure-notes {
            background: #f0f8ff;
            padding: 10px;
            margin: 10px 0;
            border-radius: 6px;
            font-style: italic;
            color: #34495e;
            border-left: 3px solid #3498db;
        }
        
        .download-section {
            text-align: center;
            margin-top: 30px;
            padding: 20px;
            background: #f8f9fa;
            border-radius: 10px;
        }
        
        .loading-spinner {
            display: inline-block;
            width: 20px;
            height: 20px;
            border: 3px solid #f3f3f3;
            border-top: 3px solid #3498db;
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }
        
        @keyframes spin {
            0% { transform: rotate(0deg); }
            100% { transform: rotate(360deg); }
        }

        .language-toggle {
            text-align: center;
            margin: 20px 0;
        }
        
        .language-toggle button {
            margin: 0 10px;
            padding: 8px 16px;
            border: 2px solid #3498db;
            background: white;
            color: #3498db;
            border-radius: 20px;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        
        .language-toggle button.active {
            background: #3498db;
            color: white;
        }

        .voice-settings {
            background: #e8f4fd;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #0078d4;
        }

        .voice-settings select {
            margin: 5px 10px;
            padding: 5px;
            border-radius: 4px;
            border: 1px solid #ddd;
        }

        .debug-section {
            background: #f8f9fa;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #dc3545;
            font-family: monospace;
            font-size: 12px;
            max-height: 200px;
            overflow-y: auto;
        }

        .debug-section h4 {
            color: #dc3545;
            margin-top: 0;
        }

        .debug-log {
            background: #f8f9fa;
            padding: 5px;
            margin: 5px 0;
            border-radius: 4px;
            color: #6c757d;
        }

        .debug-error {
            background: #f8d7da;
            color: #721c24;
        }

        .debug-info {
            background: #d1ecf1;
            color: #0c5460;
        }

        .troubleshooting {
            background: #fff3cd;
            padding: 15px;
            border-radius: 8px;
            margin: 15px 0;
            border-left: 4px solid #ffc107;
        }

        .troubleshooting h4 {
            color: #856404;
            margin-top: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ¥ è­·ç†å­¸ç”Ÿå‚·å£è­·ç†è¨“ç·´ç³»çµ± <span class="azure-badge">Azure AI Powered</span></h1>
            <h2>Professional Nursing Student Wound Care Training System</h2>
            <p>Advanced AI-powered patient simulation with realistic Cantonese voices</p>
        </div>
        
        <div class="scenario-info">
            <h3>ğŸ“‹ è¨“ç·´å ´æ™¯ Training Scenario</h3>
            <div class="patient-info">
                <h4>ğŸ‘¨ ç—…äººè³‡æ–™ Patient Information:</h4>
                <p><strong>å§“å Name:</strong> æ–‡åŠ›æ–°å…ˆç”Ÿ (Mr. Man Lik Sun)</p>
                <p><strong>HN Number:</strong> 23332099</p>
                <p><strong>å¹´é½¡ Age:</strong> 65æ­²</p>
                <p><strong>èªè¨€ Language:</strong> å»£æ±è©±/è‹±æ–‡ (Cantonese/English Mix)</p>
                <p><strong>ç—…æƒ… Condition:</strong> è…¿éƒ¨å‚·å£éœ€è¦æ¸…æ´—åŠæ›è—¥ (Leg wound requiring cleaning and dressing)</p>
            </div>
            <p><strong>ä½ çš„è§’è‰² Your Role:</strong> XXå¤§å­¸è­·ç†å­¸ç”Ÿä½•åŒå­¸ (Nursing Student Ho from XX University)</p>
            <p><strong>ä»»å‹™ Task:</strong> é€²è¡Œå‚·å£è­·ç†ç¨‹åºä¸¦èˆ‡ç—…äººä¿æŒè‰¯å¥½æºé€š</p>
            <p><strong>é‡é» Focus:</strong> èº«ä»½ç¢ºèªã€ç¨‹åºè§£é‡‹ã€ç—…äººå®‰æ’«ã€å°ˆæ¥­æºé€š</p>
        </div>

        <div class="voice-settings">
            <h4>ğŸ™ï¸ Voice & Recognition Settings</h4>
            <label>Patient Voice: 
                <select id="voiceSelect">
                    <option value="zh-HK-HiuMaanNeural">HiuMaan (Female, Elderly)</option>
                    <option value="zh-HK-WanLungNeural">WanLung (Male, Elderly)</option>
                    <option value="zh-HK-HiuGaaiNeural">HiuGaai (Female, Young)</option>
                </select>
            </label>
            <label>Speech Recognition: 
                <select id="recognitionLang">
                    <option value="zh-HK">å»£æ±è©± Cantonese (Hong Kong)</option>
                    <option value="en-US">English (US)</option>
                    <option value="zh-CN">æ™®é€šè©± Mandarin</option>
                </select>
            </label>
        </div>

        <div id="troubleshooting" class="troubleshooting" style="display: none;">
            <h4>ğŸ”§ Troubleshooting - å¦‚æœé‡åˆ° 404 éŒ¯èª¤</h4>
            <p><strong>å¸¸è¦‹å•é¡Œ:</strong></p>
            <ul>
                <li>Netlify å‡½æ•¸æœªæ­£ç¢ºéƒ¨ç½²</li>
                <li>ç¼ºå°‘ package.json æ–‡ä»¶</li>
                <li>Azure API å¯†é‘°æœªè¨­ç½®</li>
            </ul>
            <p><strong>è§£æ±ºæ­¥é©Ÿ:</strong></p>
            <ol>
                <li>ç¢ºä¿æ ¹ç›®éŒ„æœ‰ package.json æ–‡ä»¶</li>
                <li>é‹è¡Œ <code>npm install</code></li>
                <li>åœ¨ Netlify ç’°å¢ƒè®Šé‡ä¸­è¨­ç½® <code>AZURE_SPEECH_KEY</code></li>
                <li>é‡æ–°éƒ¨ç½²é …ç›®</li>
            </ol>
            <button onclick="testConnection()" class="btn btn-primary">é‡æ–°æ¸¬è©¦é€£æ¥</button>
        </div>

        <div id="debugSection" class="debug-section" style="display: none;">
            <h4>ğŸ”§ Debug Information</h4>
            <div id="debugLogs"></div>
            <button onclick="clearDebugLogs()" class="btn btn-primary" style="font-size: 12px; padding: 5px 10px;">Clear Logs</button>
            <button onclick="toggleDebug()" class="btn btn-primary" style="font-size: 12px; padding: 5px 10px;">Hide Debug</button>
        </div>

        <div class="controls">
            <button onclick="toggleDebug()" class="btn btn-primary">ğŸ”§ Show Debug</button>
            <button onclick="toggleTroubleshooting()" class="btn btn-primary">ğŸ“‹ Troubleshooting</button>
        </div>
        
        <div id="status" class="status loading">
            <span class="loading-spinner"></span> æ­£åœ¨åˆå§‹åŒ–AzureèªéŸ³æœå‹™... Initializing Azure Speech Services...
        </div>
        
        <div class="controls">
            <button id="startBtn" class="btn btn-primary" disabled>ğŸ™ï¸ é–‹å§‹è¨“ç·´ Start Training</button>
            <button id="stopBtn" class="btn btn-danger" disabled>â¹ï¸ çµæŸè¨“ç·´ Stop Training</button>
        </div>
        
        <div id="timer" class="timer">00:00</div>
        
        <div id="conversation" class="conversation" style="display: none;">
            <h4>ğŸ’¬ å°è©±è¨˜éŒ„ Session Conversation:</h4>
            <div id="messages"></div>
        </div>
        
        <div id="downloadSection" class="download-section" style="display: none;">
            <h4>ğŸ“¥ è¨“ç·´è¨˜éŒ„ Session Recording</h4>
            <p>ä½ çš„è¨“ç·´å°è©±å·²è¢«è¨˜éŒ„ä»¥ä¾›åˆ†æ Your training session has been recorded for analysis.</p>
            <button id="downloadBtn" class="btn btn-primary">ä¸‹è¼‰éŒ„éŸ³ Download Recording</button>
        </div>
    </div>

    <script type="module">
        // Netlify Function Configuration - å¤šä¸ªç«¯ç‚¹å°è¯•
        const SPEECH_ENDPOINTS = [
            '/.netlify/functions/speech',
            '/api/speech',
            'https://your-site-name.netlify.app/.netlify/functions/speech'
        ];
        
        // æ£€æµ‹å½“å‰ä½¿ç”¨çš„ç«¯ç‚¹
        let CURRENT_ENDPOINT = SPEECH_ENDPOINTS[0];
        
        // Global variables
        let mediaRecorder;
        let audioChunks = [];
        let recognition;
        let sessionStartTime;
        let timerInterval;
        let isRecording = false;
        let currentStep = 0;
        let conversationHistory = [];
        let selectedVoice = 'zh-HK-HiuMaanNeural';
        let selectedRecognitionLang = 'zh-HK';
        let debugMode = false;
        
        // Debug logging function
        function debugLog(message, type = 'info') {
            console.log(`[${type.toUpperCase()}] ${message}`);
            
            if (debugMode) {
                const debugLogs = document.getElementById('debugLogs');
                const logDiv = document.createElement('div');
                logDiv.className = `debug-log debug-${type}`;
                logDiv.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;
                debugLogs.appendChild(logDiv);
                debugLogs.scrollTop = debugLogs.scrollHeight;
            }
        }

        function clearDebugLogs() {
            document.getElementById('debugLogs').innerHTML = '';
        }

        function toggleDebug() {
            debugMode = !debugMode;
            const debugSection = document.getElementById('debugSection');
            debugSection.style.display = debugMode ? 'block' : 'none';
            
            const toggleBtn = document.querySelector('button[onclick="toggleDebug()"]');
            toggleBtn.textContent = debugMode ? 'ğŸ”§ Hide Debug' : 'ğŸ”§ Show Debug';
        }

        function toggleTroubleshooting() {
            const troubleshootingSection = document.getElementById('troubleshooting');
            const isVisible = troubleshootingSection.style.display !== 'none';
            troubleshootingSection.style.display = isVisible ? 'none' : 'block';
        }

        // æµ‹è¯•è¿æ¥å‡½æ•°
        async function testConnection() {
            debugLog('Testing all available endpoints...');
            
            for (let i = 0; i < SPEECH_ENDPOINTS.length; i++) {
                const endpoint = SPEECH_ENDPOINTS[i];
                debugLog(`Testing endpoint ${i + 1}: ${endpoint}`);
                
                try {
                    const response = await fetch(endpoint, {
                        method: 'OPTIONS'
                    });
                    
                    debugLog(`Endpoint ${i + 1} OPTIONS response: ${response.status}`);
                    
                    if (response.status === 200 || response.status === 204) {
                        CURRENT_ENDPOINT = endpoint;
                        debugLog(`âœ… Found working endpoint: ${endpoint}`, 'info');
                        return endpoint;
                    }
                } catch (error) {
                    debugLog(`âŒ Endpoint ${i + 1} failed: ${error.message}`, 'error');
                }
            }
            
            throw new Error('æ‰€æœ‰ç«¯é»éƒ½ç„¡æ³•é€£æ¥ All endpoints failed');
        }
        
        // Realistic nursing procedure flow with mixed Cantonese-English responses
        const nursingProcedure = [
            {
                phase: "introduction",
                expectedInput: ["æˆ‘ä¿‚", "è­·å£«å­¸ç”Ÿ", "ä½•", "å…¥åšŸ"],
                patientResponse: `<speak version="1.0" xml:lang="zh-HK">
                    <voice name="{voice}">
                        <mstts:express-as style="cheerful">
                            å¥½å•Šï¼Œå…¥åšŸå•¦ã€‚ä½ å«å’©åå‘€ï¼Ÿ
                            <lang xml:lang="en-US">What is your name?</lang>
                        </mstts:express-as>
                    </voice>
                </speak>`,
                notes: "Student should introduce themselves and ask permission to enter"
            },
            {
                phase: "identification",
                expectedInput: ["æ–‡ç”Ÿ", "å°å", "æ–‡åŠ›æ–°", "23332099"],
                patientResponse: `<speak version="1.0" xml:lang="zh-HK">
                    <voice name="{voice}">
                        ä¿‚å•Šï¼Œæˆ‘ä¿‚æ–‡åŠ›æ–°ã€‚ä½ è¦åšå’©å‘€ï¼Ÿ
                        <lang xml:lang="en-US">My leg is hurt.</lang>
                        æœ‰å•²ç—›ã—ã€‚
                    </voice>
                </speak>`,
                notes: "Patient identification verification completed"
            },
            {
                phase: "explanation",
                expectedInput: ["æ´—å‚·å£", "æ›è—¥", "å””å¥½éƒ"],
                patientResponse: `<speak version="1.0" xml:lang="zh-HK">
                    <voice name="{voice}">
                        <mstts:express-as style="fearful">
                            æ´—å‚·å£ï¼Ÿæœƒå””æœƒå¥½ç—›ã—ï¼Ÿ
                            <lang xml:lang="en-US">I am scared.</lang>
                            æˆ‘æœ‰å•²é©šå‘€ã€‚
                        </mstts:express-as>
                    </voice>
                </speak>`,
                notes: "Student explains wound care procedure"
            },
            {
                phase: "positioning",
                expectedInput: ["èˆ’æœ", "å§¿å‹¢", "ä¿æŒ"],
                patientResponse: `<speak version="1.0" xml:lang="zh-HK">
                    <voice name="{voice}">
                        å¥½å•¦ï¼Œæˆ‘ç›¡é‡å””éƒã€‚
                        <lang xml:lang="en-US">But I am nervous.</lang>
                        ä¸éçœŸä¿‚æœ‰å•²ç·Šå¼µã€‚
                    </voice>
                </speak>`,
                notes: "Actions: Organize quilt, position patient comfortably, prepare equipment"
            },
            {
                phase: "procedure",
                expectedInput: ["è€Œå®¶", "é–‹å§‹", "æ´—", "æ¸…æ½”"],
                patientResponse: `<speak version="1.0" xml:lang="zh-HK">
                    <voice name="{voice}">
                        <mstts:express-as style="fearful">
                            å—±ï¼æœ‰å•²æ¶¼å‘€ï¼
                            <lang xml:lang="en-US">So cold!</lang>
                            ä½ å°å¿ƒå•²å•¦ã€‚
                        </mstts:express-as>
                    </voice>
                </speak>`,
                notes: "Wound cleaning in progress - patient expresses mild discomfort"
            },
            {
                phase: "reassurance",
                expectedInput: ["å””èˆ’æœ", "å‡ºè²", "éš¨æ™‚", "ç—›"],
                patientResponse: `<speak version="1.0" xml:lang="zh-HK">
                    <voice name="{voice}">
                        å¥½å•¦ï¼Œå°‘å°‘ç—›ä¿‚å’ã—å•¦ã€‚
                        <lang xml:lang="en-US">What are you doing now?</lang>
                        ä½ åšç·Šå’©å‘€ï¼Ÿ
                    </voice>
                </speak>`,
                notes: "Student provides reassurance during procedure"
            },
            {
                phase: "dressing",
                expectedInput: ["åŸ‹", "å‚·å£", "åŒ…ç´®"],
                patientResponse: `<speak version="1.0" xml:lang="zh-HK">
                    <voice name="{voice}">
                        <mstts:express-as style="cheerful">
                            è€Œå®¶å¥½å•²å–‡ï¼Œå””ä¿‚å’ç—›ã€‚
                            <lang xml:lang="en-US">When do I come back?</lang>
                            å¹¾æ™‚è¦æ›ã—ï¼Ÿ
                        </mstts:express-as>
                    </voice>
                </speak>`,
                notes: "Wound dressing application"
            },
            {
                phase: "completion",
                expectedInput: ["åŸ·å˜¢", "è½‰é ­", "è¿”åšŸç‡"],
                patientResponse: `<speak version="1.0" xml:lang="zh-HK">
                    <voice name="{voice}">
                        <mstts:express-as style="grateful">
                            å¥½å•Šï¼Œå””è©²æ™’ä½ å‘€ã€‚
                            <lang xml:lang="en-US">Thank you very much.</lang>
                            è¨˜ä½è½‰é ­è¿”åšŸç‡æˆ‘å•¦ã€‚
                        </mstts:express-as>
                    </voice>
                </speak>`,
                notes: "Tidy patient's clothes, lower bed, organize equipment"
            }
        ];
        
        // DOM elements
        const statusEl = document.getElementById('status');
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const timerEl = document.getElementById('timer');
        const conversationEl = document.getElementById('conversation');
        const messagesEl = document.getElementById('messages');
        const downloadSection = document.getElementById('downloadSection');
        const downloadBtn = document.getElementById('downloadBtn');
        const voiceSelect = document.getElementById('voiceSelect');
        const recognitionLangSelect = document.getElementById('recognitionLang');
        
        // Voice and language selection
        voiceSelect.addEventListener('change', (e) => {
            selectedVoice = e.target.value;
            debugLog(`Voice changed to: ${selectedVoice}`);
        });
        
        recognitionLangSelect.addEventListener('change', (e) => {
            selectedRecognitionLang = e.target.value;
            debugLog(`Recognition language changed to: ${selectedRecognitionLang}`);
            if (recognition) {
                recognition.lang = selectedRecognitionLang;
            }
        });
        
        // Initialize the application
        async function init() {
            try {
                debugLog('Starting initialization...');
                updateStatus('æ­£åœ¨æ¸¬è©¦Netlifyé€£æ¥... Testing Netlify function connection...', 'loading');
                
                // Test connection to find working endpoint
                debugLog('Testing endpoint connections...');
                await testConnection();
                debugLog(`Using endpoint: ${CURRENT_ENDPOINT}`);
                
                updateStatus('æ­£åœ¨æ¸¬è©¦Azureé€£æ¥... Testing Azure connection...', 'loading');
                debugLog('Testing Azure connection...');
                await testAzureConnection();
                debugLog('Azure connection test successful');
                
                updateStatus('æ­£åœ¨è¼‰å…¥èªéŸ³è­˜åˆ¥... Loading speech recognition...', 'loading');
                debugLog('Initializing speech recognition...');
                await initSpeechRecognition();
                debugLog('Speech recognition initialized successfully');
                
                updateStatus('ğŸ‰ Azure AIç³»çµ±æº–å‚™å°±ç·’ï¼Ready to start professional training!', 'ready');
                startBtn.disabled = false;
                debugLog('Initialization completed successfully');
                
            } catch (error) {
                console.error('Initialization error:', error);
                debugLog(`Initialization failed: ${error.message}`, 'error');
                updateStatus(`âš ï¸ åˆå§‹åŒ–å¤±æ•— Initialization failed: ${error.message}`, 'error');
                
                // Show troubleshooting section on error
                document.getElementById('troubleshooting').style.display = 'block';
            }
        }
        
        // Test Azure connection with better error handling
        async function testAzureConnection() {
            const testSSML = `<speak version="1.0" xml:lang="zh-HK">
                <voice name="zh-HK-HiuMaanNeural">æ¸¬è©¦</voice>
            </speak>`;
            
            debugLog(`Testing connection to: ${CURRENT_ENDPOINT}`);
            debugLog(`Test SSML: ${testSSML}`);
            
            try {
                const response = await fetch(CURRENT_ENDPOINT, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ ssml: testSSML })
                });
                
                debugLog(`Response status: ${response.status}`);
                debugLog(`Response headers: ${JSON.stringify([...response.headers.entries()])}`);
                
                if (!response.ok) {
                    const errorText = await response.text();
                    debugLog(`Response error text: ${errorText}`, 'error');
                    
                    // Handle 404 specifically
                    if (response.status === 404) {
                        throw new Error(`Netlify function not found (404). Please check:\n1. netlify/functions/speech.js exists\n2. package.json with node-fetch dependency\n3. Correct build configuration\n4. AZURE_SPEECH_KEY environment variable set`);
                    }
                    
                    throw new Error(`Netlify function failed: ${response.status} - ${errorText}`);
                }
                
                const responseData = await response.json();
                debugLog(`Response data keys: ${Object.keys(responseData)}`);
                
                if (!responseData.audio) {
                    throw new Error('No audio data in response');
                }
                
                debugLog('Azure connection test successful - audio data received');
                
            } catch (error) {
                debugLog(`Connection test failed: ${error.message}`, 'error');
                throw error;
            }
        }
        
        // Initialize speech recognition
        async function initSpeechRecognition() {
            if ('webkitSpeechRecognition' in window) {
                recognition = new webkitSpeechRecognition();
                debugLog('Using webkitSpeechRecognition');
            } else if ('SpeechRecognition' in window) {
                recognition = new SpeechRecognition();
                debugLog('Using SpeechRecognition');
            } else {
                throw new Error('Speech recognition not supported in this browser');
            }
            
            recognition.continuous = true;
            recognition.interimResults = true;
            recognition.lang = selectedRecognitionLang;
            
            recognition.onresult = function(event) {
                const result = event.results[event.results.length - 1];
                if (result.isFinal) {
                    const transcript = result[0].transcript.trim();
                    debugLog(`Speech recognized: ${transcript}`);
                    addMessage('student', transcript);
                    conversationHistory.push({role: 'student', content: transcript});
                    processStudentInput(transcript);
                }
            };
            
            recognition.onerror = function(event) {
                debugLog(`Speech recognition error: ${event.error}`, 'error');
                if (event.error === 'no-speech' && isRecording) {
                    setTimeout(() => {
                        try {
                            recognition.start();
                            debugLog('Speech recognition restarted after no-speech');
                        } catch (e) {
                            debugLog(`Recognition restart failed: ${e.message}`, 'error');
                        }
                    }, 1000);
                }
            };
            
            recognition.onend = function() {
                debugLog('Speech recognition ended');
                if (isRecording) {
                    try {
                        recognition.start();
                        debugLog('Speech recognition restarted');
                    } catch (e) {
                        debugLog(`Recognition restart failed: ${e.message}`, 'error');
                    }
                }
            };
            
            debugLog('Speech recognition configured successfully');
        }
        
        // Process student input and generate appropriate patient response
        function processStudentInput(transcript) {
            debugLog(`Processing student input: ${transcript}`);
            const lowerTranscript = transcript.toLowerCase();
            let matchedPhase = null;
            let response = null;
            
            // Find the most appropriate response based on current conversation context
            for (let i = 0; i < nursingProcedure.length; i++) {
                const phase = nursingProcedure[i];
                
                // Check if any expected keywords are present
                const hasKeywords = phase.expectedInput.some(keyword => 
                    lowerTranscript.includes(keyword.toLowerCase())
                );
                
                if (hasKeywords) {
                    matchedPhase = phase;
                    response = phase.patientResponse.replace('{voice}', selectedVoice);
                    debugLog(`Matched phase: ${phase.phase}`);
                    break;
                }
            }
            
            // Fallback responses for unmatched input
            if (!response) {
                const fallbackResponses = [
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}">å’©è©±ï¼Ÿæˆ‘è½å””æ¸…æ¥šå‘€ã€‚<lang xml:lang="en-US">What did you say?</lang></voice></speak>`,
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}">ä½ å¯å””å¯ä»¥è¬›æ…¢å•²ï¼Ÿ<lang xml:lang="en-US">Can you speak slower?</lang></voice></speak>`,
                    `<speak version="1.0" xml:lang="zh-HK"><voice name="${selectedVoice}">æˆ‘å””ä¿‚å¥½æ˜ç™½ä½ è¬›å’©ã€‚<lang xml:lang="en-US">I don't understand.</lang></voice></speak>`
                ];
                response = fallbackResponses[Math.floor(Math.random() * fallbackResponses.length)];
                debugLog('Using fallback response');
            }
            
            // Add patient response with delay for realism
            setTimeout(() => {
                const responseText = extractTextFromSSML(response);
                debugLog(`Patient responding: ${responseText}`);
                addMessage('patient', responseText, matchedPhase?.notes);
                conversationHistory.push({role: 'patient', content: responseText});
                speakPatientResponseAzure(response);
            }, 1500 + Math.random() * 1000);
        }
        
        // Extract text content from SSML for display
        function extractTextFromSSML(ssml) {
            const tempDiv = document.createElement('div');
            tempDiv.innerHTML = ssml.replace(/<[^>]*>/g, ' ').replace(/\s+/g, ' ').trim();
            return tempDiv.textContent;
        }
        
        // Secure Azure Text-to-Speech via Netlify Function
        async function speakPatientResponseAzure(ssmlText) {
            try {
                debugLog('Sending TTS request to Azure via Netlify function');
                debugLog(`SSML length: ${ssmlText.length} characters`);
                debugLog(`Using endpoint: ${CURRENT_ENDPOINT}`);
                
                const response = await fetch(CURRENT_ENDPOINT, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({ ssml: ssmlText })
                });
                
                debugLog(`TTS response status: ${response.status}`);
                
                if (response.ok) {
                    const data = await response.json();
                    debugLog('TTS response received successfully');
                    debugLog(`Audio data length: ${data.audio ? data.audio.length : 'undefined'} characters`);
                    
                    // Convert base64 back to audio
                    const audioBlob = new Blob(
                        [Uint8Array.from(atob(data.audio), c => c.charCodeAt(0))], 
                        { type: 'audio/mpeg' }
                    );
                    const audioUrl = URL.createObjectURL(audioBlob);
                    const audio = new Audio(audioUrl);
                    
                    audio.oncanplay = () => debugLog('Audio ready to play');
                    audio.onerror = (e) => debugLog(`Audio playback error: ${e.message}`, 'error');
                    
                    await audio.play();
                    debugLog('Audio playback started');
                } else {
                    const errorData = await response.json();
                    debugLog(`TTS function failed: ${response.status}`, 'error');
                    debugLog(`Error details: ${JSON.stringify(errorData)}`, 'error');
                }
            } catch (error) {
                debugLog(`TTS function error: ${error.message}`, 'error');
                console.error('Speech function error:', error);
            }
        }
        
        // Add message to conversation
        function addMessage(speaker, text, notes = null) {
            const messageDiv = document.createElement('div');
            messageDiv.className = `message ${speaker}-message`;
            
            const header = document.createElement('div');
            header.className = 'message-header';
            header.innerHTML = speaker === 'student' ? 
                'ğŸ‘©â€âš•ï¸ è­·ç†å­¸ç”Ÿä½•åŒå­¸ Student Ho' : 
                'ğŸ‘¨ ç—…äººæ–‡å…ˆç”Ÿ Patient Mr. Man';
            
            const content = document.createElement('div');
            content.className = 'message-content';
            content.innerHTML = `<span class="cantonese-text">${text}</span>`;
            
            messageDiv.appendChild(header);
            messageDiv.appendChild(content);
            
            if (notes && speaker === 'student') {
                const notesDiv = document.createElement('div');
                notesDiv.className = 'procedure-notes';
                notesDiv.textContent = `ğŸ“ ${notes}`;
                messageDiv.appendChild(notesDiv);
            }
            
            messagesEl.appendChild(messageDiv);
            messagesEl.scrollTop = messagesEl.scrollHeight;
        }
        
        // Start training session
        async function startSession() {
            try {
                debugLog('Starting training session');
                isRecording = true;
                currentStep = 0;
                audioChunks = [];
                conversationHistory = [];
                
                // Clear previous conversation
                messagesEl.innerHTML = '';
                conversationEl.style.display = 'block';
                
                // Start audio recording
                debugLog('Requesting microphone access');
                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                debugLog('Microphone access granted');
                
                mediaRecorder = new MediaRecorder(stream);
                
                mediaRecorder.ondataavailable = function(event) {
                    audioChunks.push(event.data);
                };
                
                mediaRecorder.onstop = function() {
                    debugLog('Media recorder stopped, creating download link');
                    const audioBlob = new Blob(audioChunks, { type: 'audio/wav' });
                    createDownloadLink(audioBlob);
                };
                
                mediaRecorder.start();
                debugLog('Audio recording started');
                
                // Start speech recognition
                debugLog('Starting speech recognition');
                recognition.start();
                
                // Start timer
                sessionStartTime = Date.now();
                startTimer();
                
                // Initial scenario setup with Azure TTS
                setTimeout(() => {
                    debugLog('Playing initial patient greeting');
                    const initialSSML = `<speak version="1.0" xml:lang="zh-HK">
                        <voice name="${selectedVoice}">
                            <mstts:express-as style="curious">
                                å‘€ï¼Ÿæœ‰äººåšŸå–‡ã€‚é‚Šå€‹å‘€ï¼Ÿ
                                <lang xml:lang="en-US">Who is there?</lang>
                            </mstts:express-as>
                        </voice>
                    </speak>`;
                    
                    addMessage('patient', "å‘€ï¼Ÿæœ‰äººåšŸå–‡ã€‚é‚Šå€‹å‘€ï¼ŸWho is there?", "Patient notices someone approaching the room");
                    speakPatientResponseAzure(initialSSML);
                }, 3000);
                
                updateStatus('ğŸ™ï¸ å°ˆæ¥­éŒ„éŸ³é€²è¡Œä¸­... Professional AI training in progress', 'recording');
                startBtn.disabled = true;
                stopBtn.disabled = false;
                debugLog('Training session started successfully');
                
            } catch (error) {
                debugLog(`Error starting session: ${error.message}`, 'error');
                console.error('Error starting session:', error);
                alert('ç„¡æ³•ä½¿ç”¨éº¥å…‹é¢¨ï¼Œè«‹æª¢æŸ¥æ¬Šé™è¨­å®š Error accessing microphone. Please check permissions.');
            }
        }
        
        // Stop training session
        function stopSession() {
            debugLog('Stopping training session');
            isRecording = false;
            
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
                debugLog('Media recorder stopped');
            }
            
            if (recognition) {
                recognition.stop();
                debugLog('Speech recognition stopped');
            }
            
            stopTimer();
            
            updateStatus('âœ… å°ˆæ¥­è¨“ç·´å®Œæˆï¼Professional training completed! Review your session below.', 'ready');
            startBtn.disabled = false;
            stopBtn.disabled = true;
            
            downloadSection.style.display = 'block';
            debugLog('Training session stopped successfully');
        }
        
        // Timer functions
        function startTimer() {
            timerInterval = setInterval(() => {
                const elapsed = Date.now() - sessionStartTime;
                const minutes = Math.floor(elapsed / 60000);
                const seconds = Math.floor((elapsed % 60000) / 1000);
                timerEl.textContent = `${minutes.toString().padStart(2, '0')}:${seconds.toString().padStart(2, '0')}`;
            }, 1000);
        }
        
        function stopTimer() {
            if (timerInterval) {
                clearInterval(timerInterval);
            }
        }
        
        // Create download link for recording
        function createDownloadLink(audioBlob) {
            const url = URL.createObjectURL(audioBlob);
            downloadBtn.onclick = function() {
                const a = document.createElement('a');
                a.href = url;
                a.download = `azure-nursing-training-${new Date().toISOString().slice(0, 19)}.wav`;
                a.click();
            };
        }
        
        // Update status display
        function updateStatus(message, type) {
            statusEl.textContent = message;
            statusEl.className = `status ${type}`;
        }
        
        // Make testConnection available globally
        window.testConnection = testConnection;
        
        // Event listeners
        startBtn.addEventListener('click', startSession);
        stopBtn.addEventListener('click', stopSession);
        
        // Initialize on page load
        init();
    </script>
</body>
</html>